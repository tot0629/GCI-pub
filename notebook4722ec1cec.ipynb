{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":6988458,"sourceType":"datasetVersion","datasetId":4016523},{"sourceId":151227664,"sourceType":"kernelVersion"}],"dockerImageVersionId":30587,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-11-19T10:38:01.357242Z","iopub.execute_input":"2023-11-19T10:38:01.358177Z","iopub.status.idle":"2023-11-19T10:38:01.781391Z","shell.execute_reply.started":"2023-11-19T10:38:01.358134Z","shell.execute_reply":"2023-11-19T10:38:01.780182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\n# Load the uploaded data file\nfile_path = '/kaggle/input/conpe1/train.csv'\ndata = pd.read_csv(file_path)\n\n# Display the first few rows of the dataset to understand its structure and contents\ndata.head()\n","metadata":{"execution":{"iopub.status.busy":"2023-11-19T10:38:01.783655Z","iopub.execute_input":"2023-11-19T10:38:01.784824Z","iopub.status.idle":"2023-11-19T10:38:01.838015Z","shell.execute_reply.started":"2023-11-19T10:38:01.784782Z","shell.execute_reply":"2023-11-19T10:38:01.837187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check for missing values in the dataset\nmissing_values = data.isnull().sum()\nmissing_values[missing_values > 0]\n","metadata":{"execution":{"iopub.status.busy":"2023-11-19T10:38:01.839152Z","iopub.execute_input":"2023-11-19T10:38:01.839709Z","iopub.status.idle":"2023-11-19T10:38:01.848460Z","shell.execute_reply.started":"2023-11-19T10:38:01.839677Z","shell.execute_reply":"2023-11-19T10:38:01.847533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fill missing values\ndata['Age'].fillna(data['Age'].median(), inplace=True)\ndata['Cabin'].fillna('Unknown', inplace=True)\ndata['Embarked'].fillna(data['Embarked'].mode()[0], inplace=True)\n\n# Check if all missing values are filled\nmissing_values_after = data.isnull().sum()\nmissing_values_after[missing_values_after > 0]\n","metadata":{"execution":{"iopub.status.busy":"2023-11-19T10:38:01.851166Z","iopub.execute_input":"2023-11-19T10:38:01.851441Z","iopub.status.idle":"2023-11-19T10:38:01.868789Z","shell.execute_reply.started":"2023-11-19T10:38:01.851417Z","shell.execute_reply":"2023-11-19T10:38:01.868009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\n\n# Create new feature: Family Size\ndata['FamilySize'] = data['SibSp'] + data['Parch'] + 1\n\n# Extract Title from Name\ndata['Title'] = data['Name'].apply(lambda x: re.search(' ([A-Za-z]+)\\.', x).group(1))\n\n# Simplify the titles\ntitle_mappings = {\"Mr\": \"Mr\", \"Miss\": \"Miss\", \"Mrs\": \"Mrs\", \n                  \"Master\": \"Master\", \"Dr\": \"Officer\", \"Rev\": \"Officer\", \n                  \"Col\": \"Officer\", \"Major\": \"Officer\", \"Mlle\": \"Miss\", \n                  \"Countess\": \"Royalty\", \"Ms\": \"Mrs\", \"Lady\": \"Royalty\", \n                  \"Jonkheer\": \"Royalty\", \"Don\": \"Royalty\", \"Sir\" : \"Royalty\", \n                  \"Mme\": \"Mrs\", \"Capt\": \"Officer\", \"Dona\": \"Royalty\"}\ndata['Title'] = data['Title'].map(title_mappings)\n\n# Check the new columns\ndata[['FamilySize', 'Title']].head()\n","metadata":{"execution":{"iopub.status.busy":"2023-11-19T10:38:01.869888Z","iopub.execute_input":"2023-11-19T10:38:01.870138Z","iopub.status.idle":"2023-11-19T10:38:01.891586Z","shell.execute_reply.started":"2023-11-19T10:38:01.870117Z","shell.execute_reply":"2023-11-19T10:38:01.890840Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create new feature: Cabin Type (using the first letter of the Cabin)\ndata['CabinType'] = data['Cabin'].apply(lambda x: x[0])\n\n# Check the new column\ndata['CabinType'].value_counts()\n","metadata":{"execution":{"iopub.status.busy":"2023-11-19T10:38:01.892548Z","iopub.execute_input":"2023-11-19T10:38:01.893151Z","iopub.status.idle":"2023-11-19T10:38:01.908252Z","shell.execute_reply.started":"2023-11-19T10:38:01.893123Z","shell.execute_reply":"2023-11-19T10:38:01.907678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Plot the distribution of 'Fare'\nplt.figure(figsize=(10, 6))\nplt.hist(data['Fare'], bins=40, color='blue', edgecolor='black')\nplt.title('Distribution of Fare')\nplt.xlabel('Fare')\nplt.ylabel('Frequency')\nplt.show()\n\n# Plot the distribution of 'Age'\nplt.figure(figsize=(10, 6))\nplt.hist(data['Age'], bins=40, color='green', edgecolor='black')\nplt.title('Distribution of Age')\nplt.xlabel('Age')\nplt.ylabel('Frequency')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-11-19T10:38:01.909169Z","iopub.execute_input":"2023-11-19T10:38:01.909895Z","iopub.status.idle":"2023-11-19T10:38:02.654119Z","shell.execute_reply.started":"2023-11-19T10:38:01.909869Z","shell.execute_reply":"2023-11-19T10:38:02.653441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Categorize 'Fare' into different bins\ndata['FareCategory'] = pd.cut(data['Fare'], bins=[0, 10, 20, 30, 100, float('inf')], labels=[1, 2, 3, 4, 5])\n\n# Categorize 'Age' into different bins\ndata['AgeCategory'] = pd.cut(data['Age'], bins=[0, 12, 18, 30, 50, float('inf')], labels=[1, 2, 3, 4, 5])\n\n# Check the new features\ndata[['FareCategory', 'AgeCategory']].head()\n","metadata":{"execution":{"iopub.status.busy":"2023-11-19T10:38:02.655008Z","iopub.execute_input":"2023-11-19T10:38:02.655607Z","iopub.status.idle":"2023-11-19T10:38:02.673837Z","shell.execute_reply.started":"2023-11-19T10:38:02.655581Z","shell.execute_reply":"2023-11-19T10:38:02.672943Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Display the final form of the dataset\ndata.head()\n","metadata":{"execution":{"iopub.status.busy":"2023-11-19T10:38:02.674945Z","iopub.execute_input":"2023-11-19T10:38:02.675218Z","iopub.status.idle":"2023-11-19T10:38:02.697139Z","shell.execute_reply.started":"2023-11-19T10:38:02.675194Z","shell.execute_reply":"2023-11-19T10:38:02.696479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Splitting the training data into features and target variable\nX_train = data.drop(['PassengerId', 'Perished', 'Name', 'Ticket', 'Cabin'], axis=1)\ny_train = data['Perished']\n\n# Load the test data\ntest_data_path = '/kaggle/input/conpe1/test.csv'\ntest_data = pd.read_csv(test_data_path)\n\n# Apply the same preprocessing to the test data\ntest_data['Age'].fillna(test_data['Age'].median(), inplace=True)\ntest_data['Cabin'].fillna('Unknown', inplace=True)\ntest_data['Embarked'].fillna(test_data['Embarked'].mode()[0], inplace=True)\ntest_data = pd.get_dummies(test_data, columns=['Sex', 'Embarked'])\ntest_data['FamilySize'] = test_data['SibSp'] + test_data['Parch'] + 1\ntest_data['Title'] = test_data['Name'].apply(lambda x: re.search(' ([A-Za-z]+)\\.', x).group(1))\ntest_data['Title'] = test_data['Title'].map(title_mappings)\ntest_data['CabinType'] = test_data['Cabin'].apply(lambda x: x[0])\ntest_data['FareCategory'] = pd.cut(test_data['Fare'], bins=[0, 10, 20, 30, 100, float('inf')], labels=[1, 2, 3, 4, 5])\ntest_data['AgeCategory'] = pd.cut(test_data['Age'], bins=[0, 12, 18, 30, 50, float('inf')], labels=[1, 2, 3, 4, 5])\nX_test = test_data.drop(['PassengerId', 'Name', 'Ticket', 'Cabin'], axis=1)\n\n# Display the first few rows of the processed test data\nX_test.head()\n","metadata":{"execution":{"iopub.status.busy":"2023-11-19T10:38:02.700712Z","iopub.execute_input":"2023-11-19T10:38:02.701259Z","iopub.status.idle":"2023-11-19T10:38:02.758234Z","shell.execute_reply.started":"2023-11-19T10:38:02.701228Z","shell.execute_reply":"2023-11-19T10:38:02.757526Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.head()","metadata":{"execution":{"iopub.status.busy":"2023-11-19T10:38:02.759518Z","iopub.execute_input":"2023-11-19T10:38:02.760153Z","iopub.status.idle":"2023-11-19T10:38:02.779511Z","shell.execute_reply.started":"2023-11-19T10:38:02.760113Z","shell.execute_reply":"2023-11-19T10:38:02.778279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\n# 仮定：X_train は既にあなたのデータセットです。\n# ワンホットエンコーディングを適用するカラムを選択します。\ncolumns_to_encode = ['Sex', 'Embarked', 'Title', 'CabinType']\n\n# pd.get_dummiesを使用してワンホットエンコーディングを適用します。\nX_train_encoded = pd.get_dummies(X_train, columns=columns_to_encode)\n\n# エンコードされたデータセットを確認します。\nprint(X_train_encoded.head())\n\nimport pandas as pd\n\n# 仮定：X_train は既にあなたのデータセットです。\n# ワンホットエンコーディングを適用するカラムを選択します。\ncolumns_to_encode = ['Sex', 'Embarked', 'Title', 'CabinType']\n\n# pd.get_dummiesを使用してワンホットエンコーディングを適用します。\ny_train_encoded = pd.get_dummies(y_train, columns=columns_to_encode)\n\n# エンコードされたデータセットを確認します。\nprint(y_train_encoded.head())\n\n","metadata":{"execution":{"iopub.status.busy":"2023-11-19T10:38:02.783340Z","iopub.execute_input":"2023-11-19T10:38:02.783702Z","iopub.status.idle":"2023-11-19T10:38:02.810054Z","shell.execute_reply.started":"2023-11-19T10:38:02.783673Z","shell.execute_reply":"2023-11-19T10:38:02.809090Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 'Perished' カラムを作成し、死亡した場合は 1、生きている場合は 0 とします\ny_train = y_train_encoded.apply(lambda x: 1 if x[0] else 0, axis=1)\n\ny_train.head()","metadata":{"execution":{"iopub.status.busy":"2023-11-19T10:38:02.811259Z","iopub.execute_input":"2023-11-19T10:38:02.812203Z","iopub.status.idle":"2023-11-19T10:38:02.831255Z","shell.execute_reply.started":"2023-11-19T10:38:02.812163Z","shell.execute_reply":"2023-11-19T10:38:02.830282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 各列に含まれる NaN 値の数を確認\nnan_counts = X_train_encoded.isna().sum()\nX_train_encoded.FareCategory.head()","metadata":{"execution":{"iopub.status.busy":"2023-11-19T10:38:02.833186Z","iopub.execute_input":"2023-11-19T10:38:02.833587Z","iopub.status.idle":"2023-11-19T10:38:02.844895Z","shell.execute_reply.started":"2023-11-19T10:38:02.833551Z","shell.execute_reply":"2023-11-19T10:38:02.843795Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# FareCategory の最頻値を計算\nfare_category_mode = X_train_encoded['FareCategory'].mode()[0]\n\n# FareCategory の欠損値を最頻値で置換\nX_train_encoded['FareCategory'].fillna(fare_category_mode, inplace=True)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-19T10:38:02.848216Z","iopub.execute_input":"2023-11-19T10:38:02.848527Z","iopub.status.idle":"2023-11-19T10:38:02.859967Z","shell.execute_reply.started":"2023-11-19T10:38:02.848475Z","shell.execute_reply":"2023-11-19T10:38:02.858906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 'Perished' カラムを作成し、死亡（True）の場合は 1、そうでない場合は 0 とする\ny_train = y_train_encoded.apply(lambda x: 1 if x[0] else 0, axis=1)\ny_train.head()","metadata":{"execution":{"iopub.status.busy":"2023-11-19T10:38:02.862943Z","iopub.execute_input":"2023-11-19T10:38:02.863580Z","iopub.status.idle":"2023-11-19T10:38:02.883147Z","shell.execute_reply.started":"2023-11-19T10:38:02.863543Z","shell.execute_reply":"2023-11-19T10:38:02.882061Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import GradientBoostingClassifier\n\n# Initialize the models\nmodels = {\n    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n    \"Random Forest\": RandomForestClassifier(n_estimators=100),\n    \"Support Vector Machine\": SVC(),\n    \"Gradient Boosting\": GradientBoostingClassifier()\n}\n\n# Perform cross-validation and store the results\n# Perform cross-validation and store the results\nmodel_scores = {}\nfor name, model in models.items():\n    scores = cross_val_score(model, X_train_encoded, y_train, cv=5, scoring='accuracy')\n    model_scores[name] = scores.mean()\n\nmodel_scores\n","metadata":{"execution":{"iopub.status.busy":"2023-11-19T10:38:02.884789Z","iopub.execute_input":"2023-11-19T10:38:02.885144Z","iopub.status.idle":"2023-11-19T10:38:10.331555Z","shell.execute_reply.started":"2023-11-19T10:38:02.885110Z","shell.execute_reply":"2023-11-19T10:38:10.330715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\n# 仮定：X_train は既にあなたのデータセットです。\n# ワンホットエンコーディングを適用するカラムを選択します。\ncolumns_to_encode = ['Sex', 'Embarked', 'Title', 'CabinType']\n\n# pd.get_dummiesを使用してワンホットエンコーディングを適用します。\nX_train_encoded = pd.get_dummies(X_train, columns=columns_to_encode)\n\n# エンコードされたデータセットを確認します。\nprint(X_train_encoded.head())\n","metadata":{"execution":{"iopub.status.busy":"2023-11-19T10:38:10.332864Z","iopub.execute_input":"2023-11-19T10:38:10.333192Z","iopub.status.idle":"2023-11-19T10:38:10.353526Z","shell.execute_reply.started":"2023-11-19T10:38:10.333159Z","shell.execute_reply":"2023-11-19T10:38:10.352546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# FareCategory の最頻値を計算\nfare_category_mode = X_train_encoded['FareCategory'].mode()[0]\n\n# FareCategory の欠損値を最頻値で置換\nX_train_encoded['FareCategory'].fillna(fare_category_mode, inplace=True)\n\n# 再度 NaN 値の有無を確認\nnan_counts = X_train_encoded.isna().sum()\nprint(nan_counts)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-19T10:38:10.354899Z","iopub.execute_input":"2023-11-19T10:38:10.355170Z","iopub.status.idle":"2023-11-19T10:38:10.363897Z","shell.execute_reply.started":"2023-11-19T10:38:10.355147Z","shell.execute_reply":"2023-11-19T10:38:10.362652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test.columns","metadata":{"execution":{"iopub.status.busy":"2023-11-19T10:38:10.365397Z","iopub.execute_input":"2023-11-19T10:38:10.365756Z","iopub.status.idle":"2023-11-19T10:38:10.375696Z","shell.execute_reply.started":"2023-11-19T10:38:10.365728Z","shell.execute_reply":"2023-11-19T10:38:10.374329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_encoded.columns","metadata":{"execution":{"iopub.status.busy":"2023-11-19T10:38:10.377682Z","iopub.execute_input":"2023-11-19T10:38:10.378536Z","iopub.status.idle":"2023-11-19T10:38:10.386969Z","shell.execute_reply.started":"2023-11-19T10:38:10.378481Z","shell.execute_reply":"2023-11-19T10:38:10.385815Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# テストデータにワンホットエンコーディングを適用\nX_test_encoded = pd.get_dummies(X_test)\n\n# トレーニングデータにあってテストデータにないカラムを追加（値は0で埋める）\nfor column in X_train_encoded.columns:\n    if column not in X_test_encoded.columns:\n        X_test_encoded[column] = 0\n\n# テストデータにあってトレーニングデータにないカラムを削除\nX_test_encoded = X_test_encoded[X_train_encoded.columns]\n\n# エンコードされたテストデータセットを確認\nprint(X_test_encoded.head())\n","metadata":{"execution":{"iopub.status.busy":"2023-11-19T10:38:10.388596Z","iopub.execute_input":"2023-11-19T10:38:10.388951Z","iopub.status.idle":"2023-11-19T10:38:10.413256Z","shell.execute_reply.started":"2023-11-19T10:38:10.388917Z","shell.execute_reply":"2023-11-19T10:38:10.412130Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nan_counts = X_test_encoded.isna().sum()\nprint(nan_counts)","metadata":{"execution":{"iopub.status.busy":"2023-11-19T10:38:10.415020Z","iopub.execute_input":"2023-11-19T10:38:10.415398Z","iopub.status.idle":"2023-11-19T10:38:10.423766Z","shell.execute_reply.started":"2023-11-19T10:38:10.415363Z","shell.execute_reply":"2023-11-19T10:38:10.422581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test_encoded.fillna(X_test_encoded.Fare.median(),inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-11-19T10:38:10.425065Z","iopub.execute_input":"2023-11-19T10:38:10.425319Z","iopub.status.idle":"2023-11-19T10:38:10.430899Z","shell.execute_reply.started":"2023-11-19T10:38:10.425297Z","shell.execute_reply":"2023-11-19T10:38:10.430011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train the Gradient Boosting model on the entire training set using the encoded data\ngradient_boosting_model = GradientBoostingClassifier()\ngradient_boosting_model.fit(X_train_encoded, y_train)\n\n# Make predictions on the encoded test set\npredictions = gradient_boosting_model.predict(X_test_encoded)\n\n# Display the first few predictions\npredictions[:10]\n","metadata":{"execution":{"iopub.status.busy":"2023-11-19T10:38:10.432200Z","iopub.execute_input":"2023-11-19T10:38:10.432516Z","iopub.status.idle":"2023-11-19T10:38:10.635968Z","shell.execute_reply.started":"2023-11-19T10:38:10.432467Z","shell.execute_reply":"2023-11-19T10:38:10.634270Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions.shape","metadata":{"execution":{"iopub.status.busy":"2023-11-19T10:38:10.638321Z","iopub.execute_input":"2023-11-19T10:38:10.638892Z","iopub.status.idle":"2023-11-19T10:38:10.644904Z","shell.execute_reply.started":"2023-11-19T10:38:10.638861Z","shell.execute_reply":"2023-11-19T10:38:10.643897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test.head()","metadata":{"execution":{"iopub.status.busy":"2023-11-19T10:38:10.646526Z","iopub.execute_input":"2023-11-19T10:38:10.646916Z","iopub.status.idle":"2023-11-19T10:38:10.668625Z","shell.execute_reply.started":"2023-11-19T10:38:10.646882Z","shell.execute_reply":"2023-11-19T10:38:10.667743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head()","metadata":{"execution":{"iopub.status.busy":"2023-11-19T10:38:10.673568Z","iopub.execute_input":"2023-11-19T10:38:10.673863Z","iopub.status.idle":"2023-11-19T10:38:10.693188Z","shell.execute_reply.started":"2023-11-19T10:38:10.673821Z","shell.execute_reply":"2023-11-19T10:38:10.692171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# DataFrameを作成（ここでは'id'をテストデータセットのIDカラム名と仮定）\nsubmission = pd.DataFrame({\n    \"PassengerId\": data_pred_pre.PassengerId,\n    'Perished': predictions\n})\n\n# CSVファイルに保存\nsubmission.to_csv('submission_gpt.csv', index=False)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-19T10:38:32.303034Z","iopub.execute_input":"2023-11-19T10:38:32.303443Z","iopub.status.idle":"2023-11-19T10:38:32.313257Z","shell.execute_reply.started":"2023-11-19T10:38:32.303391Z","shell.execute_reply":"2023-11-19T10:38:32.312545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.shape","metadata":{"execution":{"iopub.status.busy":"2023-11-19T10:38:11.145860Z","iopub.status.idle":"2023-11-19T10:38:11.146241Z","shell.execute_reply.started":"2023-11-19T10:38:11.146056Z","shell.execute_reply":"2023-11-19T10:38:11.146075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_pred_pre=pd.read_csv('/kaggle/input/conpe1/test.csv')","metadata":{"execution":{"iopub.status.busy":"2023-11-19T10:38:28.964570Z","iopub.execute_input":"2023-11-19T10:38:28.964986Z","iopub.status.idle":"2023-11-19T10:38:28.974366Z","shell.execute_reply.started":"2023-11-19T10:38:28.964952Z","shell.execute_reply":"2023-11-19T10:38:28.973564Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_pred_pre.shape","metadata":{"execution":{"iopub.status.busy":"2023-11-19T10:38:11.149109Z","iopub.status.idle":"2023-11-19T10:38:11.149447Z","shell.execute_reply.started":"2023-11-19T10:38:11.149275Z","shell.execute_reply":"2023-11-19T10:38:11.149292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}