{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1925,"sourceType":"datasetVersion","datasetId":1067}],"dockerImageVersionId":30301,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# <h1 style=\"font-family: Times New Roman; border-radius : 10px;padding: 25px; font-size: 40px; color: #FCF6F5; text-align: center; line-height: 0.50;background-color: #2BAE66\"><b>IBM HR Analytics</b><br></h1>","metadata":{}},{"cell_type":"markdown","source":"<center>\n    <img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQoKWCb0545g__QBdCLP8_7IUmIjC2GFZtzBQ&usqp=CAU\" alt=\"IBM HR Analytics\" width=\"50%\">\n</center>\n\n### Problem Statement :\n\nIBM HR Analytics is a dataset with more than 30 features that are categorical and discrete with numerical and text data. With the emergence of storing data in digital format as well as recognising it's value, the race of automating the number of outdated systems to improve speed  accuracy is on! Thus, this fictional dataset gives us an opportunity to automate employee hiring & firing system of an organization. This is possible with the help of Data Science & Machine Learning techniques.      \n  \n### Aim :\n- To classify / predict whether an employee continues with the company or not!\n- To draw insights about employee performance as well as employee retention / departure from the employee data!","metadata":{}},{"cell_type":"markdown","source":"### Dataset Attributes :\n    \n- **Age** : Numerical Discrete Data\n- **Attrition** : Text Categorical Data\n- **BusinessTravel** : Text Categorical Data\n- **DailyRate** : Numerical Discrete Data\n- **Department** : Text Categorical Data\n- **DistanceFromHome** : Numerical Discrete Data\n- **Education** : Numerical Categorical Data\n    - **1 : Below College**\n    - **2 : College**\n    - **3 : Bachelor**\n    - **4 : Master**\n    - **5 : Doctor**\n- **EducationField** : Text Categorical Data\n- **EmployeeCount** : Numerical Categorical Data\n- **EmployeeNumber** : Numerical Categorical Data\n- **EnvironmentSatisfaction** : Numerical Categorical Data\n    - **1 : Low**\n    - **2 : Medium**\n    - **3 : High**\n    - **4 : Very High**\n- **Gender** : Text Categorical Data\n- **HourlyRate** : Numerical Discrete Data\n- **JobInvolvement** : Numerical Categorical Data\n    - **1 : Low**\n    - **2 : Medium**\n    - **3 : High**\n    - **4 : Very High**\n- **JobLevel** : Numerical Categorical Data\n- **JobRole** : Text Categorical Data\n- **JobSatisfaction** : Numerical Categorical Data\n    - **1 : Low**\n    - **2 : Medium**\n    - **3 : High**\n    - **4 : Very High**\n- **MaritalStatus** : Text Categorical Data\n- **MonthlyIncome** : Numerical Discrete Data\n- **MonthlyRate** : Numerical Discrete Data\n- **NumCompaniesWorked** : Numerical Discrete Data\n- **Over18** : Text Categorical Data\n- **OverTime** : Text Categorical Data\n- **PercentSalaryHike** : Numerical Discrete Data\n- **PerformanceRating** : Numerical Categorical Data\n    - **1 : Low**\n    - **2 : Good**\n    - **3 : Excellent**\n    - **4 : Outstanding**\n- **RelationshipSatisfaction** : Numerical Categorical Data\n    - **1 : Low**\n    - **2 : Medium**\n    - **3 : High**\n    - **4 : Very High**\n- **StandardHours** : Numerical Discrete Data\n- **StockOptionLevel** : Numerical Categorical Data\n- **TotalWorkingYears** : Numerical Discrete Data\n- **TrainingTimesLastYear** : Numerical Discrete Data\n- **WorkLifeBalance** : Numerical Categorical Data\n    - **1 : Bad**\n    - **2 : Good**\n    - **3 : Better**\n    - **4 : Best**\n- **YearsAtCompany** : Numerical Discete Data\n- **YearsInCurrentRole** : Numerical Discrete Data\n- **YearsSinceLastPromotion** : Numerical Discrete Data\n- **YearsCurrManager** : Numerical Discrete Data","metadata":{}},{"cell_type":"markdown","source":"### Notebook Contents :\n- Dataset Information\n- Exploratory Data Analysis (EDA)\n- Summary of EDA\n- Feature Engineering (Data Balancing & Data Leakage)\n- Modeling\n- Conclusion\n\n### What you will learn :\n- Data Visualization\n- Data Balancing using SMOTE\n- Data Leakage\n- Statistical Tests for Feature Selection\n- Modeling and visualization of results for algorithms\n\n### Lets get started!","metadata":{}},{"cell_type":"markdown","source":"# <center><div style=\"font-family: Times New Roman; border-radius : 10px; background-color: #2BAE66; color: #FCF6F5; padding: 12px; line-height: 1;\">Dataset Information</div></center>","metadata":{}},{"cell_type":"markdown","source":"### Import the Necessary Libraries :","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\npd.options.display.float_format = '{:.2f}'.format\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom tqdm import tqdm\nfrom sklearn.preprocessing import LabelEncoder\nfrom datetime import datetime","metadata":{"execution":{"iopub.status.busy":"2023-11-23T02:17:41.416454Z","iopub.execute_input":"2023-11-23T02:17:41.417027Z","iopub.status.idle":"2023-11-23T02:17:42.870157Z","shell.execute_reply.started":"2023-11-23T02:17:41.416918Z","shell.execute_reply":"2023-11-23T02:17:42.868871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/ibm-hr-analytics-attrition-dataset/WA_Fn-UseC_-HR-Employee-Attrition.csv')\ndata.head()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-11-23T02:17:42.873579Z","iopub.execute_input":"2023-11-23T02:17:42.874458Z","iopub.status.idle":"2023-11-23T02:17:42.948799Z","shell.execute_reply.started":"2023-11-23T02:17:42.874414Z","shell.execute_reply":"2023-11-23T02:17:42.947255Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data Info :","metadata":{}},{"cell_type":"code","source":"data.shape","metadata":{"execution":{"iopub.status.busy":"2023-11-23T02:17:42.950313Z","iopub.execute_input":"2023-11-23T02:17:42.950767Z","iopub.status.idle":"2023-11-23T02:17:42.960390Z","shell.execute_reply.started":"2023-11-23T02:17:42.950711Z","shell.execute_reply":"2023-11-23T02:17:42.959340Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.columns","metadata":{"execution":{"iopub.status.busy":"2023-11-23T02:17:42.963364Z","iopub.execute_input":"2023-11-23T02:17:42.964270Z","iopub.status.idle":"2023-11-23T02:17:42.975882Z","shell.execute_reply.started":"2023-11-23T02:17:42.964220Z","shell.execute_reply":"2023-11-23T02:17:42.974560Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.info()","metadata":{"execution":{"iopub.status.busy":"2023-11-23T02:17:42.977945Z","iopub.execute_input":"2023-11-23T02:17:42.978369Z","iopub.status.idle":"2023-11-23T02:17:43.011230Z","shell.execute_reply.started":"2023-11-23T02:17:42.978336Z","shell.execute_reply":"2023-11-23T02:17:43.009462Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.heatmap(data.isnull(),cmap = 'magma',cbar = False)","metadata":{"execution":{"iopub.status.busy":"2023-11-23T02:21:41.261915Z","iopub.execute_input":"2023-11-23T02:21:41.262416Z","iopub.status.idle":"2023-11-23T02:21:41.820682Z","shell.execute_reply.started":"2023-11-23T02:21:41.262331Z","shell.execute_reply":"2023-11-23T02:21:41.819230Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- **No null values** present in the data!","metadata":{}},{"cell_type":"code","source":"data.describe()","metadata":{"execution":{"iopub.status.busy":"2023-11-23T02:17:43.589910Z","iopub.execute_input":"2023-11-23T02:17:43.590820Z","iopub.status.idle":"2023-11-23T02:17:43.677107Z","shell.execute_reply.started":"2023-11-23T02:17:43.590783Z","shell.execute_reply":"2023-11-23T02:17:43.675590Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"yes = data[data['Attrition'] == 'Yes'].describe().T\nno = data[data['Attrition'] == 'No'].describe().T\n\ncolors = ['#2BAE66','#FCF6F5']\n\nfig,ax = plt.subplots(nrows = 1,ncols = 2,figsize = (10,10))\nplt.subplot(1,2,1)\nsns.heatmap(yes[['mean']],annot = True,cmap = colors,linewidths = 0.4,linecolor = 'black',cbar = False,fmt = '.2f')\nplt.title('Mean Values : Attrited Employees');\n\nplt.subplot(1,2,2)\nsns.heatmap(no[['mean']],annot = True,cmap = colors,linewidths = 0.4,linecolor = 'black',cbar = False,fmt = '.2f')\nplt.title('Mean Values : Retained Employees');\n\nfig.tight_layout(pad = 2)","metadata":{"execution":{"iopub.status.busy":"2023-11-23T02:17:43.679017Z","iopub.execute_input":"2023-11-23T02:17:43.679724Z","iopub.status.idle":"2023-11-23T02:17:45.285743Z","shell.execute_reply.started":"2023-11-23T02:17:43.679687Z","shell.execute_reply":"2023-11-23T02:17:45.284415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- **Mean** values of all the features for cases of **Attrited Employees** and **Retained Employees**.\n- When considering **age**, mean values of **staying employees** is **37.56** i.e more than the **departing employess**, **33.61**.\n- Similarly, **DailyRate** & **JobLevel** is higher for **staying employees** than **departing employees**.\n- **Staying employees** have higher values for features : **TotalWorkingYears**, **YearsAtCompany**, **YearsInCurrentRole** & **YearsWithCurrManager**.","metadata":{}},{"cell_type":"markdown","source":"# <center><div style=\"font-family: Times New Roman; border-radius : 10px; background-color: #2BAE66; color: #FCF6F5; padding: 12px; line-height: 1;\">Exploratory Data Analysis</div></center>","metadata":{}},{"cell_type":"markdown","source":"### Dividing features into Numerical and Categorical :","metadata":{}},{"cell_type":"code","source":"discrete_features = ['Age', 'DailyRate', 'DistanceFromHome', 'HourlyRate', 'MonthlyIncome', 'MonthlyRate', 'NumCompaniesWorked',\n                 'PercentSalaryHike', 'StandardHours', 'TotalWorkingYears', 'TrainingTimesLastYear', 'YearsAtCompany', \n                 'YearsInCurrentRole', 'YearsSinceLastPromotion', 'YearsWithCurrManager']\ncategorical_features = ['Attrition', 'BusinessTravel','Department', 'Education', 'EducationField', 'EmployeeCount','EmployeeNumber',\n                    'EnvironmentSatisfaction', 'Gender', 'JobInvolvement', 'JobLevel', 'JobRole', 'JobSatisfaction',\n                    'MaritalStatus', 'Over18', 'OverTime', 'PerformanceRating', 'RelationshipSatisfaction', 'StockOptionLevel',\n                    'WorkLifeBalance']\n\ndf1 = data.copy(deep = True)","metadata":{"execution":{"iopub.status.busy":"2023-11-23T02:17:45.287185Z","iopub.execute_input":"2023-11-23T02:17:45.287561Z","iopub.status.idle":"2023-11-23T02:17:45.298742Z","shell.execute_reply.started":"2023-11-23T02:17:45.287525Z","shell.execute_reply":"2023-11-23T02:17:45.296934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- According to the dataset information, we divide the features into **categorical and discrete features**.\n- Typical approach for this division of features can also be based on the datatypes of the elements of the respective \nattribute.\n\n**Eg :** datatype = integer, attribute = numerical feature ; datatype = string, attribute = categorical feature\n\n- Creating a deep copy of the orginal dataset for experimenting with data, visualization and modeling.\n- Modifications in the original dataset will not be highlighted in this deep copy.\n- We now LabelEncode the categorical features with text data.","metadata":{}},{"cell_type":"code","source":"le = LabelEncoder()\nl1 = []; l2 = []; text_categorical_features = []\nprint('Label Encoder Transformation')\nfor i in tqdm(categorical_features):\n    if type(df1[i][0]) == str:\n        text_categorical_features.append(i)\n        df1[i] = le.fit_transform(df1[i])\n        l1.append(list(df1[i].unique())); l2.append(list(le.inverse_transform(df1[i].unique())))\n        print(i,' : ',df1[i].unique(),' = ',le.inverse_transform(df1[i].unique()))","metadata":{"execution":{"iopub.status.busy":"2023-11-23T02:17:45.304722Z","iopub.execute_input":"2023-11-23T02:17:45.305284Z","iopub.status.idle":"2023-11-23T02:17:45.347156Z","shell.execute_reply.started":"2023-11-23T02:17:45.305237Z","shell.execute_reply":"2023-11-23T02:17:45.345879Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- We store the label encoded transformations inside a dictionary that gives us the information about the encoded value and it's original value! ","metadata":{}},{"cell_type":"code","source":"tf1 = {}\nfor i in range(len(text_categorical_features)):\n    tf1[text_categorical_features[i]] = {}\n    for j,k in zip(l1[i],l2[i]):\n        tf1[text_categorical_features[i]][j] = k","metadata":{"execution":{"iopub.status.busy":"2023-11-23T02:17:45.348675Z","iopub.execute_input":"2023-11-23T02:17:45.349044Z","iopub.status.idle":"2023-11-23T02:17:45.356738Z","shell.execute_reply.started":"2023-11-23T02:17:45.349008Z","shell.execute_reply":"2023-11-23T02:17:45.355526Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Categorical Features :\n\n#### Distribution of Categorical Features :","metadata":{}},{"cell_type":"code","source":"for i in range(5):\n    fig, ax = plt.subplots(nrows = 1,ncols = 4,figsize = (15,3))\n    a = 1\n    for j in categorical_features[(i*4) : (i*4) + 4]:\n        plt.subplot(1,4,a) \n        sns.distplot(df1[j],kde_kws = {'bw' : 1},color = colors[0]);\n        plt.title('Distribution : ' + j)\n        a += 1","metadata":{"execution":{"iopub.status.busy":"2023-11-23T02:17:45.359172Z","iopub.execute_input":"2023-11-23T02:17:45.359603Z","iopub.status.idle":"2023-11-23T02:17:50.533374Z","shell.execute_reply.started":"2023-11-23T02:17:45.359549Z","shell.execute_reply":"2023-11-23T02:17:50.531687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- **EmployeeNumber** is a just a unique identifying number with no repetitive elements. Hence, we will drop this feature.\n- A **bimodal** distribution can be observed for **JobRole**.\n- A lot of features have slight **rightly** & **leftly** skewed data distribution.\n- **Over18** & **EmployeeCount** are single value features.\n- We now drop the redundant features from the dataframe as well as from the list of categorical features. We also drop the **Attrition** feature as it is the target variable & will consider it separately.","metadata":{}},{"cell_type":"code","source":"df1.drop(columns = ['EmployeeCount', 'EmployeeNumber', 'Over18'], inplace = True)\ncategorical_features.remove('EmployeeCount'); categorical_features.remove('EmployeeNumber') \ncategorical_features.remove('Over18'); categorical_features.remove('Attrition')","metadata":{"execution":{"iopub.status.busy":"2023-11-23T02:17:50.535494Z","iopub.execute_input":"2023-11-23T02:17:50.536040Z","iopub.status.idle":"2023-11-23T02:17:50.548849Z","shell.execute_reply.started":"2023-11-23T02:17:50.535986Z","shell.execute_reply":"2023-11-23T02:17:50.547024Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Discrete Features :\n\n#### Distribution of Discrete Features :","metadata":{}},{"cell_type":"code","source":"for i in range(5):\n    fig, ax = plt.subplots(nrows = 1,ncols = 3,figsize = (15,3))\n    a = 1\n    for j in discrete_features[(i*3) : (i*3) + 3]:\n        plt.subplot(1,3,a) \n        sns.distplot(df1[j],kde_kws = {'bw' : 1},color = colors[0]);\n        plt.title('Distribution : ' + j)\n        a += 1","metadata":{"execution":{"iopub.status.busy":"2023-11-23T02:17:50.550900Z","iopub.execute_input":"2023-11-23T02:17:50.551365Z","iopub.status.idle":"2023-11-23T02:17:54.576255Z","shell.execute_reply.started":"2023-11-23T02:17:50.551329Z","shell.execute_reply":"2023-11-23T02:17:54.574581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- **HourlyRate**, **DailyRate** & **MonthlyRate** display graphs that are usually found in **Time Series**. These graphs  display values w.r.t time.\n- **DistanceFromHome**, **MonthlyIncome**, **NumCompaniesWorked**, **PercentSalaryHike**, **TotalWorkingYears**, **YearsAtCompany** & **YearsSinceLastPromotion** display a **rightly-skewed** data distribution.\n- **YearsInCurrentRole** & **YearsWithCurrManager** have a **bimodal** data distribution. **StandardHours** is a single value feature.\n- We now drop the redundant features from the dataframe as well as from the list of discrete features.","metadata":{}},{"cell_type":"code","source":"df1.drop(columns = ['StandardHours'], inplace = True)\ndiscrete_features.remove('StandardHours')","metadata":{"execution":{"iopub.status.busy":"2023-11-23T02:17:54.578199Z","iopub.execute_input":"2023-11-23T02:17:54.578673Z","iopub.status.idle":"2023-11-23T02:17:54.587293Z","shell.execute_reply.started":"2023-11-23T02:17:54.578633Z","shell.execute_reply":"2023-11-23T02:17:54.585778Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Target Variable Visualization (Attrition) : ","metadata":{}},{"cell_type":"code","source":"l = list(df1['Attrition'].value_counts())\ncircle = [l[0] / sum(l) * 100,l[1] / sum(l) * 100]\n\nfig = plt.subplots(nrows = 1,ncols = 2,figsize = (20,5))\nplt.subplot(1,2,1)\nplt.pie(circle,labels = list(tf1['Attrition'][j] for j in sorted(df1['Attrition'].unique())),autopct='%1.1f%%',startangle = 90,explode = (0.1,0),colors = colors,\n       wedgeprops = {'edgecolor' : 'black','linewidth': 1,'antialiased' : True})\nplt.title('Attrited Employee (%)');\n\nplt.subplot(1,2,2)\nax = sns.countplot('Attrition',data = df1, palette = colors,edgecolor = 'black')\nfor rect in ax.patches:\n    ax.text(rect.get_x() + rect.get_width() / 2, rect.get_height() + 2, rect.get_height(), horizontalalignment='center', fontsize = 11)\nax.set_xticklabels(tf1['Attrition'][j] for j in sorted(df1['Attrition'].unique()))\nplt.title('Number of Attrited Employees');\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-11-23T02:17:54.589648Z","iopub.execute_input":"2023-11-23T02:17:54.590048Z","iopub.status.idle":"2023-11-23T02:17:54.914648Z","shell.execute_reply.started":"2023-11-23T02:17:54.590014Z","shell.execute_reply":"2023-11-23T02:17:54.913262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- The dataset is **highly unbalanced**!\n- **5.2 : 1** ratio for **Retained Employees : Attrited Employee** is found!\n- Due to this, predictions will be biased towards **Retention** cases.\n- Visualizations will also display this bias, thus making it difficult to gain insight.","metadata":{}},{"cell_type":"markdown","source":"- After dropping the single value features and removing the target feature, **Attrition**, we group the remaining 30 features according to their characteristics & by intuition. \n- They are divided into 5 groups as follows :\n    - **General Employee Information :**\n    - **Employee - Job Information**\n    - **Employe - Company Information**\n    - **Company Features**\n    - **Finances**","metadata":{}},{"cell_type":"code","source":"l1 = ['Age', 'Gender','MaritalStatus', 'Education', \n      'DistanceFromHome', 'TotalWorkingYears', 'NumCompaniesWorked'] # General Employee Information\n\nl2 = ['EducationField', 'Department', 'JobLevel', 'JobRole', \n      'JobInvolvement', 'OverTime', 'JobSatisfaction'] # Employee Job Information\n\nl3 = ['YearsAtCompany', 'YearsInCurrentRole', 'YearsWithCurrManager', \n      'YearsSinceLastPromotion', 'TrainingTimesLastYear', 'WorkLifeBalance'] # Employee - Company Information\n\nl4 = ['PercentSalaryHike', 'StockOptionLevel', 'BusinessTravel', \n      'PerformanceRating', 'EnvironmentSatisfaction', 'RelationshipSatisfaction'] # Company Information \n\nl5 = ['MonthlyIncome', 'HourlyRate', 'DailyRate', 'MonthlyRate'] # Finances\n\ndf2 = pd.DataFrame()\ndf2['Attrition'] = df1['Attrition']","metadata":{"execution":{"iopub.status.busy":"2023-11-23T02:17:54.916817Z","iopub.execute_input":"2023-11-23T02:17:54.918340Z","iopub.status.idle":"2023-11-23T02:17:54.929826Z","shell.execute_reply.started":"2023-11-23T02:17:54.918257Z","shell.execute_reply":"2023-11-23T02:17:54.928933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- We create a dummy dataframe with the **Attrition** feature that can be used for storing features that need to be manipulated for drawing insights  visualization purposes! ","metadata":{}},{"cell_type":"markdown","source":"**We will draw insights from the group of features by visualization techniques!**","metadata":{}},{"cell_type":"markdown","source":"### General Employee Information :\n\n- It includes features that provide information about the basic information of an employee! \n\n- List of Features :\n    - **Age**\n    - **Gender**\n    - **MaritalStatus**\n    - **Education**\n    - **DistanceFromHome**\n    - **TotalWorkingYears**\n    - **NumCompaniesWorked**","metadata":{}},{"cell_type":"code","source":"df2['Age_Group'] = [int(i/5) for i in df1['Age']]\n\nplt.figure(figsize = (15,5))\nax = sns.countplot('Age_Group', data = df2, hue = 'Attrition', palette = colors,edgecolor = 'black')\nfor rect in ax.patches:\n    ax.text(rect.get_x() + rect.get_width() / 2, rect.get_height(), rect.get_height(), horizontalalignment='center', fontsize = 11)\nplt.legend(['Retained Employees', 'Attrited Employees'], loc = 'upper right')\nplt.title('Age');","metadata":{"execution":{"iopub.status.busy":"2023-11-23T02:17:54.931757Z","iopub.execute_input":"2023-11-23T02:17:54.932256Z","iopub.status.idle":"2023-11-23T02:17:55.355532Z","shell.execute_reply.started":"2023-11-23T02:17:54.932212Z","shell.execute_reply":"2023-11-23T02:17:55.354602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- **Attrition** is present in near about all the age groups. \n- For **Age** values between **30 - 34**, highest number of employees, **59**, have departed. Employees with **Age** values **25 - 29** come second with **53** employees discontinuing their jobs with the company.\n- Age values from **20 - 24** & **35 - 40** near about display the same number of attrited employees with **28** & **30** respectively.\n- Employees above the age of **40** have also been relieved of their duties.","metadata":{}},{"cell_type":"code","source":"df2['DistanceFromHome_Group'] = [int(i/5) for i in df1['DistanceFromHome']]\n\nplt.figure(figsize = (15,5))\nax = sns.countplot('DistanceFromHome_Group', data = df2, hue = 'Attrition', palette = colors,edgecolor = 'black')\nfor rect in ax.patches:\n    ax.text(rect.get_x() + rect.get_width() / 2, rect.get_height(), rect.get_height(), horizontalalignment='center', fontsize = 11)\nplt.legend(['Retained Employees', 'Attrited Employees'], loc = 'upper right'); plt.title('DistanceFromHome');\n\nfor i in range(2):\n    fig = plt.subplots(nrows = 1,ncols = 3,figsize = (15,15)); a = 1\n    for j in range(3):\n        plt.subplot(1,3,a)\n        if i == 0:\n            l = list(df2.loc[(df2['DistanceFromHome_Group'] == j)]['Attrition'].value_counts())\n        else:\n            l = list(df2.loc[(df2['DistanceFromHome_Group'] == (j+3))]['Attrition'].value_counts())\n            \n        circle = [l[0] / sum(l) * 100,l[1] / sum(l) * 100]\n        plt.pie(circle,labels = list(tf1['Attrition'][k] for k in sorted(df2['Attrition'].unique())),autopct = '%1.1f%%',startangle = 90,explode = (0.1,0),\n                colors = colors, wedgeprops = {'edgecolor' : 'black','linewidth': 1,'antialiased' : True})\n        if i == 0:\n            plt.title('DistanceFromHome : ' + str(j*5) + ' - ' + str(j*5 + 4) + ' ('+ str(j) + ')');\n        else:\n            plt.title('DistanceFromHome : ' + str((j+3)*5) + ' - ' + str((j+3)*5 + 4) + ' ('+ str(j+3) + ')');\n        a += 1","metadata":{"execution":{"iopub.status.busy":"2023-11-23T02:17:55.356781Z","iopub.execute_input":"2023-11-23T02:17:55.357988Z","iopub.status.idle":"2023-11-23T02:17:56.278150Z","shell.execute_reply.started":"2023-11-23T02:17:55.357948Z","shell.execute_reply":"2023-11-23T02:17:56.276623Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- From the 1st graph, we can say that employees living nearest to the company i.e within **0 - 4**, they have been attrited the most, however when we check the percentage of attrition, it tells us a different story.\n- Employees living within the distance of **0 - 4** have been attrited the least. As the value of **DistanceFromHome** increases, employee attrition increases! ","metadata":{}},{"cell_type":"code","source":"df2['TotalWorkingYears_Group'] = [int(i/5) for i in df1['TotalWorkingYears']]\n\nplt.figure(figsize = (15,5))\nax = sns.countplot('TotalWorkingYears_Group', data = df2, hue = 'Attrition', palette = colors,edgecolor = 'black')\nfor rect in ax.patches:\n    ax.text(rect.get_x() + rect.get_width() / 2, rect.get_height(), rect.get_height(), horizontalalignment='center', fontsize = 11)\nplt.legend(['Retained Employees', 'Attrited Employees'], loc = 'upper right')\nplt.title('TotalWorkingYears');","metadata":{"execution":{"iopub.status.busy":"2023-11-23T02:17:56.280129Z","iopub.execute_input":"2023-11-23T02:17:56.283000Z","iopub.status.idle":"2023-11-23T02:17:56.633313Z","shell.execute_reply.started":"2023-11-23T02:17:56.282943Z","shell.execute_reply":"2023-11-23T02:17:56.631926Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- From the above visualization, we can say that employees **within their 1st 10 years of work experience** are highly prone to being removed! \n- As the work experience increases, chances of attrition reduces!","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize = (15,5))\nax = sns.countplot('NumCompaniesWorked', data = df1, hue = 'Attrition', palette = colors,edgecolor = 'black')\nfor rect in ax.patches:\n    ax.text(rect.get_x() + rect.get_width() / 2, rect.get_height(), rect.get_height(), horizontalalignment='center', fontsize = 11)\nplt.legend(['Retained Employees', 'Attrited Employees'], loc = 'upper right')\nplt.title('Number of Companies Worked For');","metadata":{"execution":{"iopub.status.busy":"2023-11-23T02:17:56.635473Z","iopub.execute_input":"2023-11-23T02:17:56.637229Z","iopub.status.idle":"2023-11-23T02:17:56.990669Z","shell.execute_reply.started":"2023-11-23T02:17:56.637172Z","shell.execute_reply":"2023-11-23T02:17:56.989123Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- We can clearly see that a lot of the volatility can be seen between **1st - 2nd** job.\n- This volatility gets calmed down after the 2nd job. \n- However, as the employee works in more than **4 companies**, chances of **attrition** increase drastically.","metadata":{}},{"cell_type":"code","source":"fig = plt.subplots(nrows = 1, ncols = 2, figsize = (15,5))\n\nplt.subplot(1,2,1)\nax = sns.countplot('Gender',data = df1, hue = 'Attrition', palette = colors,edgecolor = 'black')\nfor rect in ax.patches:\n    ax.text(rect.get_x() + rect.get_width() / 2, rect.get_height() + 2, rect.get_height(), horizontalalignment='center', fontsize = 11)\nax.set_xticklabels(tf1['Gender'][j] for j in sorted(df1['Gender'].unique()))\nplt.legend(['Retained Employees', 'Attrited Employees'])\nplt.title('Gender');\n\nplt.subplot(1,2,2)\nax = sns.countplot('MaritalStatus',data = df1, hue = 'Attrition', palette = colors,edgecolor = 'black')\nfor rect in ax.patches:\n    ax.text(rect.get_x() + rect.get_width() / 2, rect.get_height() + 2, rect.get_height(), horizontalalignment='center', fontsize = 11)\nax.set_xticklabels(tf1['MaritalStatus'][j] for j in sorted(df1['MaritalStatus'].unique()))\nplt.legend(['Retained Employees', 'Attrited Employees'])\nplt.title('Marital Status');","metadata":{"execution":{"iopub.status.busy":"2023-11-23T02:17:56.992762Z","iopub.execute_input":"2023-11-23T02:17:56.993245Z","iopub.status.idle":"2023-11-23T02:17:57.468282Z","shell.execute_reply.started":"2023-11-23T02:17:56.993204Z","shell.execute_reply":"2023-11-23T02:17:57.466840Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- According to the data, more **Male** employees have been removed than the **Female** employees. \n- **Single** employees have been attrited the most. **Married** employees occupy the 2nd place and **Divorced** come at the last position.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize = (15,5))\nax = sns.countplot('Education',data = df1, hue = 'Attrition', palette = colors,edgecolor = 'black')\nfor rect in ax.patches:\n    ax.text(rect.get_x() + rect.get_width() / 2, rect.get_height() + 2, rect.get_height(), horizontalalignment='center', fontsize = 11)\nax.set_xticklabels(['Below College', 'College', 'Bachelor', 'Master', 'Doctor'])\nplt.legend(['Retained Employees', 'Attrited Employees'])\nplt.title('Education');\n\nfig = plt.subplots(nrows = 1,ncols = 3,figsize = (15,15))\nfor i in range(1,4):\n    plt.subplot(1,3,i)\n    l = list(df2.loc[(df1['Education'] == i)]['Attrition'].value_counts())\n    \n    circle = [l[0] / sum(l) * 100,l[1] / sum(l) * 100]\n    plt.pie(circle,labels = list(tf1['Attrition'][k] for k in sorted(df2['Attrition'].unique())),autopct = '%1.1f%%',startangle = 90,explode = (0.1,0),\n            colors = colors, wedgeprops = {'edgecolor' : 'black','linewidth': 1,'antialiased' : True})\n    plt.title('Education : ' + ['Below College', 'College', 'Bachelor', 'Master', 'Doctor'][i-1]);\n    \nfig = plt.subplots(nrows = 1,ncols = 2,figsize = (10,10))\nfor i in range(2):\n    plt.subplot(1,2,i+1)\n    l = list(df2.loc[(df1['Education'] == (i+4))]['Attrition'].value_counts())\n    \n    circle = [l[0] / sum(l) * 100,l[1] / sum(l) * 100]\n    plt.pie(circle,labels = list(tf1['Attrition'][k] for k in sorted(df2['Attrition'].unique())),autopct = '%1.1f%%',startangle = 90,explode = (0.1,0),\n            colors = colors, wedgeprops = {'edgecolor' : 'black','linewidth': 1,'antialiased' : True})\n    plt.title('Education : ' + ['Below College', 'College', 'Bachelor', 'Master', 'Doctor'][i-2]);","metadata":{"execution":{"iopub.status.busy":"2023-11-23T02:17:57.470008Z","iopub.execute_input":"2023-11-23T02:17:57.471165Z","iopub.status.idle":"2023-11-23T02:17:58.747415Z","shell.execute_reply.started":"2023-11-23T02:17:57.471103Z","shell.execute_reply":"2023-11-23T02:17:58.745454Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Employees with **Bachelor**'s degree have been discontinued the most times followed by employees with **Master**'s degree.\n- Surprisingly, employees with **Below College** education come at the 4th rank out of 5. But, they have the **highest attrition rate**.\n- **Docter** degree employees have been attrited the least number of times & also has the **lowest attrition rate**.","metadata":{}},{"cell_type":"markdown","source":"- We will only check for **Age** vs **Gender**, **Marital Status** & **Education**!","metadata":{}},{"cell_type":"code","source":"fig = plt.subplots(nrows = 1,ncols = 2,figsize = (15,5))\nfor i in range(len(['Gender', 'MaritalStatus'])):\n    plt.subplot(1,2,i+1)\n    ax = sns.boxplot(x = ['Gender', 'MaritalStatus'][i],y = 'Age',data = df1,hue = 'Attrition',palette = colors);\n    plt.legend(['RE', 'AE'])\n    ax.set_xticklabels(tf1[['Gender', 'MaritalStatus'][i]][j] for j in sorted(df1[['Gender', 'MaritalStatus'][i]].unique()))\n    plt.title(['Gender', 'MaritalStatus'][i] + ' vs Age');\n    \nplt.figure(figsize = (15,5))\nax = sns.boxplot(x = 'Education',y = 'Age',data = df1,hue = 'Attrition',palette = colors);\nplt.legend(['Retained Employees', 'Attrited Employees'])\nax.set_xticklabels(['Below College', 'College', 'Bachelor', 'Master', 'Doctor'])\nplt.title('Education' + ' vs Age');","metadata":{"execution":{"iopub.status.busy":"2023-11-23T02:17:58.750368Z","iopub.execute_input":"2023-11-23T02:17:58.751429Z","iopub.status.idle":"2023-11-23T02:18:00.116437Z","shell.execute_reply.started":"2023-11-23T02:17:58.751344Z","shell.execute_reply":"2023-11-23T02:18:00.115059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Employees with an **Age** of less than **30** are most prone to attrition irrespective of their **Gender**, **MaritalStatus** & **Education**.\n- Both **Gender** overlap the same **Age** values somewhere **below 30 - above 35**. When it comes to **MaritalStatus**, **Single** employees have a lower limit value for **Age** than that for **Married** & **Divorced**.\n- When it comes to **Education**, employees with **College** degree have a high range of **Age** values making them highly prone to removal from company.\n- Lower limit of **Master** & **Doctor** degree employees have a high value. **Doctor** degree holders also have the least range of values making them the least targeted employees when it comes to removal of employees.","metadata":{}},{"cell_type":"markdown","source":"### Employee Job Information :\n\n- It includes features that provide information about the job &  it's characteristics!\n\n- List of Features :\n    - **EducationField**\n    - **Department**\n    - **JobLevel**\n    - **JobRole**\n    - **JobInvolvement**\n    - **OverTime**\n    - **JobSatisfaction**","metadata":{}},{"cell_type":"code","source":"fig = plt.subplots(nrows = 2,ncols = 2,figsize = (25,10))\n\nfor i in range(4):\n    plt.subplot(2,2,i+1)\n    ax = sns.countplot(['EducationField', 'Department', 'JobRole', 'OverTime'][i],data = df1, \n                       hue = 'Attrition', palette = colors,edgecolor = 'black')\n    for rect in ax.patches:\n        ax.text(rect.get_x() + rect.get_width() / 2, rect.get_height() + 2, rect.get_height(), horizontalalignment='center', fontsize = 11)\n    ax.set_xticklabels(tf1[['EducationField', 'Department', 'JobRole', 'OverTime'][i]][j] \n                       for j in sorted(df1[['EducationField', 'Department', 'JobRole', 'OverTime'][i]].unique()))\n    plt.legend(['Retained Employees', 'Attrited Employees'])\n    plt.title(['EducationField', 'Department', 'JobRole', 'OverTime'][i]);\n\nfig = plt.subplots(nrows = 1,ncols = 3,figsize = (25,5))\nplt.subplot(1,3,1)\nax = sns.countplot('JobLevel',data = df1, hue = 'Attrition', palette = colors,edgecolor = 'black')\nfor rect in ax.patches:\n    ax.text(rect.get_x() + rect.get_width() / 2, rect.get_height() + 2, rect.get_height(), horizontalalignment='center', fontsize = 11)\nplt.legend(['Retained Employees', 'Attrited Employees'])\nplt.title('JobLevel');\n\nplt.subplot(1,3,2)\nax = sns.countplot('JobInvolvement',data = df1, hue = 'Attrition', palette = colors,edgecolor = 'black')\nfor rect in ax.patches:\n    ax.text(rect.get_x() + rect.get_width() / 2, rect.get_height() + 2, rect.get_height(), horizontalalignment='center', fontsize = 11)\nax.set_xticklabels(['Low', 'Medium','High','Very High'])\nplt.legend(['Retained Employees', 'Attrited Employees'])\nplt.title('JobeInvolvement');\n\nplt.subplot(1,3,3)\nax = sns.countplot('JobSatisfaction',data = df1, hue = 'Attrition', palette = colors,edgecolor = 'black')\nfor rect in ax.patches:\n    ax.text(rect.get_x() + rect.get_width() / 2, rect.get_height() + 2, rect.get_height(), horizontalalignment='center', fontsize = 11)\nax.set_xticklabels(['Low', 'Medium', 'High','Very High'])\nplt.legend(['Retained Employees', 'Attrited Employees'])\nplt.title('JobSatisfaction');","metadata":{"execution":{"iopub.status.busy":"2023-11-23T02:18:00.118758Z","iopub.execute_input":"2023-11-23T02:18:00.119272Z","iopub.status.idle":"2023-11-23T02:18:02.265537Z","shell.execute_reply.started":"2023-11-23T02:18:00.119223Z","shell.execute_reply":"2023-11-23T02:18:02.264159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- All these graphs pretty much follow the same pattern of more the people in a category, higher is the number of removal of employees.\n- Hence, it can be deceiving as it does not call out the complete picture. Thus, we will check the attrition percentage of the individual category. ","metadata":{}},{"cell_type":"code","source":"fig = plt.subplots(nrows = 1,ncols = 6,figsize = (20,20))\nfor i in range(len(df1['EducationField'].unique())):\n    plt.subplot(1,6,i+1)\n    l = list(df1.loc[(df1['EducationField'] == i)]['Attrition'].value_counts())\n    circle = [l[0] / sum(l) * 100,l[1] / sum(l) * 100]\n    \n    plt.pie(circle,labels = list(tf1['Attrition'][k] for k in sorted(df1['Attrition'].unique())),autopct = '%1.1f%%',startangle = 90,explode = (0.1,0),\n            colors = colors, wedgeprops = {'edgecolor' : 'black','linewidth': 1,'antialiased' : True})\n    b = list(tf1['EducationField'][k] for k in sorted(df1['EducationField'].unique()))\n    plt.title('EducationField : ' + b[i]);","metadata":{"execution":{"iopub.status.busy":"2023-11-23T02:18:02.267329Z","iopub.execute_input":"2023-11-23T02:18:02.268617Z","iopub.status.idle":"2023-11-23T02:18:02.868669Z","shell.execute_reply.started":"2023-11-23T02:18:02.268563Z","shell.execute_reply":"2023-11-23T02:18:02.866976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- We can see that employees with **EducationField** of **Human Resources**, **Technical Degree** & **Marketing** have a higher chance of being removed. ","metadata":{}},{"cell_type":"code","source":"fig = plt.subplots(nrows = 1,ncols = 6,figsize = (20,20))\nc = list((sorted(df1['Department'].unique()) + sorted(df1['JobRole'].unique())[:3]))\nfor i in range(len(c)):\n    \n    plt.subplot(1,6,i+1)\n    if i < 3:\n        l = list(df1.loc[(df1['Department'] == i)]['Attrition'].value_counts())\n        circle = [l[0] / sum(l) * 100,l[1] / sum(l) * 100]\n    elif i > 2:\n        l = list(df1.loc[(df1['JobRole'] == (i - 3))]['Attrition'].value_counts())\n        circle = [l[0] / sum(l) * 100,l[1] / sum(l) * 100]\n\n    plt.pie(circle,labels = list(tf1['Attrition'][k] for k in sorted(df1['Attrition'].unique())),autopct = '%1.1f%%',startangle = 90,explode = (0.1,0),\n            colors = colors, wedgeprops = {'edgecolor' : 'black','linewidth': 1,'antialiased' : True})\n    \n    if i < 3:\n        b = list(tf1['Department'][k] for k in sorted(df1['Department'].unique()))\n        plt.title('Department : ' + b[i]);\n    elif i > 2:\n        b = list(tf1['JobRole'][k] for k in sorted(df1['JobRole'].unique()))\n        plt.title('JobRole : ' + b[i-3]);\n        \nfig = plt.subplots(nrows = 1,ncols = 6,figsize = (20,20))\n\nfor i in range(len(sorted(df1['JobRole'].unique())[3:])):\n    \n    plt.subplot(1,6,i+1)\n\n    l = list(df1.loc[(df1['JobRole'] == (i+3))]['Attrition'].value_counts())\n    circle = [l[0] / sum(l) * 100,l[1] / sum(l) * 100]\n    plt.pie(circle,labels = list(tf1['Attrition'][k] for k in sorted(df1['Attrition'].unique())),autopct = '%1.1f%%',startangle = 90,explode = (0.1,0),\n            colors = colors, wedgeprops = {'edgecolor' : 'black','linewidth': 1,'antialiased' : True})\n    b = list(tf1['JobRole'][k] for k in sorted(df1['JobRole'].unique()))\n    plt.title('JobRole : ' + b[i+3]);","metadata":{"execution":{"iopub.status.busy":"2023-11-23T02:18:02.870904Z","iopub.execute_input":"2023-11-23T02:18:02.871463Z","iopub.status.idle":"2023-11-23T02:18:04.386405Z","shell.execute_reply.started":"2023-11-23T02:18:02.871411Z","shell.execute_reply":"2023-11-23T02:18:04.385041Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- From the above of pie charts, **Sales** & **Human Resource** **Department** employees have a high probability of discontinuing with the company than **Research & Development**. \n- When it comes to **JobRole**, out of the 9 roles, 4 roles display less than **7%** of attrition rate whereas the remaining 5 roles have an attrition rate more than **15%**. ","metadata":{}},{"cell_type":"code","source":"fig = plt.subplots(nrows = 1, ncols = 2, figsize = (10,10))\nfor i in range(len(df1['OverTime'].unique())):\n    plt.subplot(1,2,i+1)\n    l = list(df1.loc[(df1['OverTime'] == i)]['Attrition'].value_counts())\n    circle = [l[0] / sum(l) * 100,l[1] / sum(l) * 100]\n    plt.pie(circle,labels = list(tf1['Attrition'][k] for k in sorted(df1['Attrition'].unique())),autopct = '%1.1f%%',startangle = 90,explode = (0.1,0),\n            colors = colors, wedgeprops = {'edgecolor' : 'black','linewidth': 1,'antialiased' : True})\n    b = list(tf1['OverTime'][k] for k in sorted(df1['OverTime'].unique()))\n    plt.title('OverTime : ' + b[i]);","metadata":{"execution":{"iopub.status.busy":"2023-11-23T02:18:04.396960Z","iopub.execute_input":"2023-11-23T02:18:04.397470Z","iopub.status.idle":"2023-11-23T02:18:04.635014Z","shell.execute_reply.started":"2023-11-23T02:18:04.397431Z","shell.execute_reply":"2023-11-23T02:18:04.633315Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- We can see that people that work **OverTime** are prone to be discontinued from the company! It has a **30%** attrition rate i.e very less as compared to employees that do not work **OverTime**.","metadata":{}},{"cell_type":"code","source":"fig = plt.subplots(nrows = 1,ncols = 4,figsize = (15,15))\nfor i in range(len(df1['JobInvolvement'].unique())):\n    plt.subplot(1,4,i+1)\n    l = list(df1.loc[(df1['JobInvolvement'] == (i+1))]['Attrition'].value_counts())\n    circle = [l[0] / sum(l) * 100,l[1] / sum(l) * 100]\n    plt.pie(circle,labels = list(tf1['Attrition'][k] for k in sorted(df1['Attrition'].unique())),autopct = '%1.1f%%',startangle = 90,explode = (0.1,0),\n            colors = colors, wedgeprops = {'edgecolor' : 'black','linewidth': 1,'antialiased' : True})\n    plt.title('JobInvolvement : ' + ['Low', 'Medium', 'High', 'Very High'][i]);\n        \nfig = plt.subplots(nrows = 1,ncols = 4,figsize = (15,15))\nfor i in range(len(df1['JobSatisfaction'].unique())):\n    plt.subplot(1,4,i+1)\n    l = list(df1.loc[(df1['JobSatisfaction'] == (i+1))]['Attrition'].value_counts())\n    circle = [l[0] / sum(l) * 100,l[1] / sum(l) * 100]\n    plt.pie(circle,labels = list(tf1['Attrition'][k] for k in sorted(df1['Attrition'].unique())),autopct = '%1.1f%%',startangle = 90,explode = (0.1,0),\n            colors = colors, wedgeprops = {'edgecolor' : 'black','linewidth': 1,'antialiased' : True})\n    plt.title('JobSatisfaction : ' + ['Low', 'Medium', 'High', 'Very High'][i]);","metadata":{"execution":{"iopub.status.busy":"2023-11-23T02:18:04.637690Z","iopub.execute_input":"2023-11-23T02:18:04.638723Z","iopub.status.idle":"2023-11-23T02:18:05.487717Z","shell.execute_reply.started":"2023-11-23T02:18:04.638663Z","shell.execute_reply":"2023-11-23T02:18:05.486078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- We can observe that higher the **JobInvolement**, lower the attrition rate!\n- Similar pattern can be observed for **JobSatisfaction**.","metadata":{}},{"cell_type":"code","source":"fig = plt.subplots(nrows = 1,ncols = 5,figsize = (15,15))\n\nfor i in range(len(df1['JobLevel'].unique())):\n    plt.subplot(1,5,i+1)\n    l = list(df1.loc[(df1['JobLevel'] == (i+1))]['Attrition'].value_counts())\n    circle = [l[0] / sum(l) * 100,l[1] / sum(l) * 100]\n    plt.pie(circle,labels = list(tf1['Attrition'][k] for k in sorted(df1['Attrition'].unique())),autopct = '%1.1f%%',startangle = 90,explode = (0.1,0),\n            colors = colors, wedgeprops = {'edgecolor' : 'black','linewidth': 1,'antialiased' : True})\n    plt.title('JobLevel : ' + str(i+1));","metadata":{"execution":{"iopub.status.busy":"2023-11-23T02:18:05.490277Z","iopub.execute_input":"2023-11-23T02:18:05.491304Z","iopub.status.idle":"2023-11-23T02:18:05.961189Z","shell.execute_reply.started":"2023-11-23T02:18:05.491232Z","shell.execute_reply":"2023-11-23T02:18:05.960296Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- **JobLevel 1** has the attrition rate with **26.3%**. **JobLevel 3** comes at the 2nd position with **14.7%**.\n- **JobLevel 4** has the lowest attrition rate with **4.7%**. \n- There seems to be no pattern. Hence, we will visualize the **JobLevel** with some features of the same group.","metadata":{}},{"cell_type":"code","source":"plt.subplots(nrows = 1, ncols = 3, figsize = (25,5))\nfor i in range(len(sorted(df1['JobLevel'].unique())[:3])):\n    plt.subplot(1,3,i+1)\n    ax = sns.countplot('JobRole',data = df1[(df1['JobLevel'] == (i+1))], palette = colors,edgecolor = 'black')\n    for rect in ax.patches:\n        ax.text(rect.get_x() + rect.get_width() / 2, rect.get_height() + 1, rect.get_height(), horizontalalignment='center', fontsize = 11)\n    ax.set_xticklabels(list(tf1['JobRole'][k] for k in sorted(df1[(df1['JobLevel'] == (i+1))]['JobRole'].unique())))\n    plt.title('JobRoles : JobLevel ' + str(i+1));\n    \nplt.subplots(nrows = 1, ncols = 2, figsize = (15,5))\nfor i in range(len(sorted(df1['JobLevel'].unique())[3:])):\n    plt.subplot(1,2,i+1)\n    ax = sns.countplot('JobRole',data = df1[(df1['JobLevel'] == (i+4))], palette = colors,edgecolor = 'black')\n    for rect in ax.patches:\n        ax.text(rect.get_x() + rect.get_width() / 2, rect.get_height() + 1, rect.get_height(), horizontalalignment='center', fontsize = 11)\n    ax.set_xticklabels(list(tf1['JobRole'][k] for k in sorted(df1[(df1['JobLevel'] == (i+4))]['JobRole'].unique())))\n    plt.title('JobRoles : JobLevel ' + str(i+4));","metadata":{"execution":{"iopub.status.busy":"2023-11-23T02:18:05.962575Z","iopub.execute_input":"2023-11-23T02:18:05.963134Z","iopub.status.idle":"2023-11-23T02:18:07.330558Z","shell.execute_reply.started":"2023-11-23T02:18:05.963073Z","shell.execute_reply":"2023-11-23T02:18:07.329403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- **JobLevel 1** has **JobRole** **Research Scientist** & **Laboratory Technician** in heavy numbers. For **JobLevel 2**, **Sales Executive** has the highest number of roles but it also has other 6 roles listed with low numbers as compared to **Sales Executive**. \n- **Sales Executive**, **Manufacturing Director** & **Healthcare Representative** take the 1st, 2nd & 3rd rank respectively in **JobLevel 3**. **Manager** roles have been found the highest number of times for **JobLevel 4**.\n- **Manager** & **Research Director** occupy the **JobRole** in **JobLevel 5**. A pattern that can be observed i.e as the **JobLevel** increases, number of **JobRole** & it's count decreases as well.","metadata":{}},{"cell_type":"markdown","source":"### Employee - Company Information :\n\n- It includes features that provide information employee's association with the company!\n\n- List of Features :\n    - **YearsAtCompany**\n    - **YearsInCurrentRole**\n    - **YearsWithCurrManager**\n    - **YearsSinceLastPromotion**\n    - **TrainingTimesLastYear**\n    - **WorkLifeBalance**","metadata":{}},{"cell_type":"code","source":"df2['YearsAtCompany_Group'] = [int(i / 5) for i in df1['YearsAtCompany']]\n\nplt.figure(figsize = (15, 5))\nax = sns.countplot('YearsAtCompany_Group', data = df2, hue = 'Attrition', palette = colors,edgecolor = 'black')\nfor rect in ax.patches:\n    ax.text(rect.get_x() + rect.get_width() / 2, rect.get_height()+0.01, rect.get_height(), horizontalalignment='center', fontsize = 11)\nplt.legend(['Retained Employees', 'Attrited Employees'])\nplt.title('YearsAtCompany');","metadata":{"execution":{"iopub.status.busy":"2023-11-23T02:18:07.332419Z","iopub.execute_input":"2023-11-23T02:18:07.333271Z","iopub.status.idle":"2023-11-23T02:18:07.742834Z","shell.execute_reply.started":"2023-11-23T02:18:07.333234Z","shell.execute_reply":"2023-11-23T02:18:07.741331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Clearly, employees that have been at the company for **0 - 4 (0)** years have been attrited the most number of times.\n- As the employees gain experience at the company, attrition reduces.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize = (15, 5))\n\nax = sns.countplot('YearsInCurrentRole', data = df1, hue = 'Attrition', palette = colors,edgecolor = 'black');\nfor rect in ax.patches:\n    ax.text(rect.get_x() + rect.get_width() / 2, rect.get_height()+0.01, rect.get_height(), horizontalalignment='center', fontsize = 11)\nplt.legend(['Retained Employees', 'Attrited Employees'])\nplt.title('YearsInCurrentRole');","metadata":{"execution":{"iopub.status.busy":"2023-11-23T02:18:07.744779Z","iopub.execute_input":"2023-11-23T02:18:07.745347Z","iopub.status.idle":"2023-11-23T02:18:08.441083Z","shell.execute_reply.started":"2023-11-23T02:18:07.745302Z","shell.execute_reply":"2023-11-23T02:18:08.439704Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- As expected by now, employees in their 1st role are very volatile and look for an early exit.\n- Another spike of attrition can also be observed when employees complete **2 years** in their current role. It looks like either employees look for improvement in their role or the companies have done evaluation, thus taking a call about the employees.\n- This is then followed by attrition in years **3** & **4**. This is probably a continuation of the attrition carried out in year 2.\n- One more significant spike can be observed in **year 7** of their current role as the employees might look for an improvement or company decides to shake up things.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize = (15, 5))\n\nax = sns.countplot('YearsWithCurrManager', data = df1, hue = 'Attrition', palette = colors,edgecolor = 'black');\nfor rect in ax.patches:\n    ax.text(rect.get_x() + rect.get_width() / 2, rect.get_height()+0.01, rect.get_height(), horizontalalignment='center', fontsize = 11)\nplt.legend(['Retained Employees', 'Attrited Employees'])\nplt.title('YearsWithCurrManager');","metadata":{"execution":{"iopub.status.busy":"2023-11-23T02:18:08.443274Z","iopub.execute_input":"2023-11-23T02:18:08.444561Z","iopub.status.idle":"2023-11-23T02:18:09.115657Z","shell.execute_reply.started":"2023-11-23T02:18:08.444499Z","shell.execute_reply":"2023-11-23T02:18:09.114008Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- This is a very similar visualization to the previous graph of **YearsInCurrentRole**. \n- Peaks of attrition can be found at the sames of : **0**, **2** & **7**.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize = (15, 5))\n\nax = sns.countplot('YearsSinceLastPromotion', data = df1, hue = 'Attrition', palette = colors,edgecolor = 'black');\nfor rect in ax.patches:\n    ax.text(rect.get_x() + rect.get_width() / 2, rect.get_height()+0.01, rect.get_height(), horizontalalignment='center', fontsize = 11)\nplt.legend(['Retained Employees', 'Attrited Employees'])\nplt.title('YearsSinceLastPromotion');","metadata":{"execution":{"iopub.status.busy":"2023-11-23T02:18:09.117369Z","iopub.execute_input":"2023-11-23T02:18:09.118018Z","iopub.status.idle":"2023-11-23T02:18:09.716700Z","shell.execute_reply.started":"2023-11-23T02:18:09.117975Z","shell.execute_reply":"2023-11-23T02:18:09.715177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- We can see that a huge number of attrition cases can be found for value **0**. I guess it's majority values represent the freshers in the company.\n- **1** & **2** years since the last promotion have also recorded a significant number of employee removal cases.\n- **7** years since last promotion also has decent number of employee removal cases. This is value seems to have some correlation with the previous 2 graphs of **YearsInCurrentRole** & **YearsWithCurrManager**.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize = (15, 5))\n\nax = sns.countplot('TrainingTimesLastYear', data = df1, hue = 'Attrition', palette = colors,edgecolor = 'black');\nfor rect in ax.patches:\n    ax.text(rect.get_x() + rect.get_width() / 2, rect.get_height()+0.01, rect.get_height(), horizontalalignment='center', fontsize = 11)\nplt.legend(['Retained Employees', 'Attrited Employees'])\nplt.title('TrainingTimesLastYear');\n\nfig = plt.subplots(nrows = 1,ncols = 7,figsize = (25,25))\n\nfor i in range(len(df1['TrainingTimesLastYear'].unique())):\n    plt.subplot(1,7,i+1)\n    l = list(df1.loc[(df1['TrainingTimesLastYear'] == (i))]['Attrition'].value_counts())\n    circle = [l[0] / sum(l) * 100,l[1] / sum(l) * 100]\n    plt.pie(circle,labels = list(tf1['Attrition'][k] for k in sorted(df1['Attrition'].unique())),autopct = '%1.1f%%',startangle = 90,explode = (0.1,0),\n            colors = colors, wedgeprops = {'edgecolor' : 'black','linewidth': 1,'antialiased' : True})\n    plt.title('TrainingTimesLastYear : ' + str(i));","metadata":{"execution":{"iopub.status.busy":"2023-11-23T02:18:09.718485Z","iopub.execute_input":"2023-11-23T02:18:09.719841Z","iopub.status.idle":"2023-11-23T02:18:10.773118Z","shell.execute_reply.started":"2023-11-23T02:18:09.719791Z","shell.execute_reply":"2023-11-23T02:18:10.772196Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Values **3** & **4** have higher values but the attrition percentage tells us a different story.\n- **TrainingTimesLastYear : 0**, **TrainingTimesLastYear : 4** & **TrainingTimesLastYear : 2** dominate the attrition percentage.\n- It looks like training is very essential as the attrition percentage is very high when no training is conducted, **27.8%**. Clearly, there is a competency problem.\n- For **TrainingTimesLastYear : 4**, attrition percentage of **21.1%** can be found which is high. Another point can be about the difficulty of the training & it's evaluation.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize = (15, 5))\n\nax = sns.countplot('WorkLifeBalance', data = df1, hue = 'Attrition', palette = colors,edgecolor = 'black');\nfor rect in ax.patches:\n    ax.text(rect.get_x() + rect.get_width() / 2, rect.get_height()+0.01, rect.get_height(), horizontalalignment='center', fontsize = 11)\nplt.legend(['Retained Employees', 'Attrited Employees'])\nax.set_xticklabels(['Bad', 'Good', 'Better', 'Best'])\nplt.title('WorkLifeBalance');\n\nfig = plt.subplots(nrows = 1,ncols = 4,figsize = (15,15))\n\nfor i in range(len(df1['WorkLifeBalance'].unique())):\n    plt.subplot(1,4,i+1)\n    l = list(df1.loc[(df1['WorkLifeBalance'] == (i+1))]['Attrition'].value_counts())\n    circle = [l[0] / sum(l) * 100,l[1] / sum(l) * 100]\n    plt.pie(circle,labels = list(tf1['Attrition'][k] for k in sorted(df1['Attrition'].unique())),autopct = '%1.1f%%',startangle = 90,explode = (0.1,0),\n            colors = colors, wedgeprops = {'edgecolor' : 'black','linewidth': 1,'antialiased' : True})\n    plt.title('WorkLifeBalance : ' + ['Bad', 'Good', 'Better', 'Best'][i]);","metadata":{"execution":{"iopub.status.busy":"2023-11-23T02:18:10.774366Z","iopub.execute_input":"2023-11-23T02:18:10.774927Z","iopub.status.idle":"2023-11-23T02:18:11.484209Z","shell.execute_reply.started":"2023-11-23T02:18:10.774892Z","shell.execute_reply":"2023-11-23T02:18:11.482588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- As expected **Bad WorkLifeBalance** has resulted in a massive attrition percentage of **31.2%**. \n- Surprisingly, **Best WorkLifeBalance** has the 2nd highest value of attrition percentage. ","metadata":{}},{"cell_type":"markdown","source":"- We will check the **WorkLifeBalance** feature with the **JobRole** & **JobLevel** features of the **Employee Job Information**!","metadata":{}},{"cell_type":"code","source":"fig = plt.subplots(nrows = 2, ncols = 2, figsize = (25,10))\nfor i in range(4):\n    plt.subplot(2,2,i+1)\n    ax = sns.countplot('JobRole', data = df1[df1['WorkLifeBalance'] == (i+1)], palette = colors, edgecolor = 'black')\n    for rect in ax.patches:\n        ax.text(rect.get_x() + rect.get_width() / 2, rect.get_height() + 0.25, rect.get_height(), horizontalalignment='center', fontsize = 11)\n    ax.set_xticklabels(tf1['JobRole'][k] for k in sorted(df1[df1['WorkLifeBalance'] == (i+1)]['JobRole'].unique()))\n    plt.legend(['Retained Employees', 'Attrited Employees'])\n    plt.title(['Bad', 'Good', 'Better', 'Best'][i] + ' WorklifeBalance of Different JobRoles');","metadata":{"execution":{"iopub.status.busy":"2023-11-23T02:18:11.487043Z","iopub.execute_input":"2023-11-23T02:18:11.487720Z","iopub.status.idle":"2023-11-23T02:18:12.994570Z","shell.execute_reply.started":"2023-11-23T02:18:11.487684Z","shell.execute_reply":"2023-11-23T02:18:12.993601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- **Laboratory Technician**, **Research Scientist** & **Sales Executive** have recorded high numbers for all the values of **WorkLifeBalance**.","metadata":{}},{"cell_type":"code","source":"fig = plt.subplots(nrows = 2, ncols = 2, figsize = (25,10))\nfor i in range(4):\n    plt.subplot(2,2,i+1)\n    ax = sns.countplot('JobLevel', data = df1[df1['WorkLifeBalance'] == (i+1)], palette = colors, edgecolor = 'black')\n    for rect in ax.patches:\n        ax.text(rect.get_x() + rect.get_width() / 2, rect.get_height() + 0.25, rect.get_height(), horizontalalignment='center', fontsize = 11)\n    ax.set_xticklabels(['JobLevel 1', 'JobLevel 2', 'JobLevel 3', 'JobLevel 4', 'JobLevel 5'])\n    plt.title(['Bad', 'Good', 'Better', 'Best'][i] + ' WorklifeBalance');","metadata":{"execution":{"iopub.status.busy":"2023-11-23T02:18:12.995968Z","iopub.execute_input":"2023-11-23T02:18:12.996357Z","iopub.status.idle":"2023-11-23T02:18:13.866248Z","shell.execute_reply.started":"2023-11-23T02:18:12.996322Z","shell.execute_reply":"2023-11-23T02:18:13.864531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- **JobLevel 1** & **JobLevel 2** record high values in for all the **WorkLifeBalance** values.","metadata":{}},{"cell_type":"markdown","source":"### Company Information :\n\n- It includes features that provide information company's characteristics w.r.t employees!\n\n- List of Features :\n    - **PercentSalaryHike**\n    - **StockOptionLevel**\n    - **BusinessTravel**\n    - **PerformanceRating**\n    - **EnvironmentSatisfaction**\n    - **RelationshipSatisfaction**","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize = (15, 5))\nax = sns.countplot('PercentSalaryHike', data = df1, hue = 'Attrition', palette = colors,edgecolor = 'black')\nfor rect in ax.patches:\n    ax.text(rect.get_x() + rect.get_width() / 2, rect.get_height()+0.01, rect.get_height(), horizontalalignment='center', fontsize = 11)\nplt.legend(['Retained Employees', 'Attrited Employees'])\nplt.title('PercentSalaryHike');","metadata":{"execution":{"iopub.status.busy":"2023-11-23T02:18:13.867796Z","iopub.execute_input":"2023-11-23T02:18:13.868439Z","iopub.status.idle":"2023-11-23T02:18:14.447995Z","shell.execute_reply.started":"2023-11-23T02:18:13.868400Z","shell.execute_reply":"2023-11-23T02:18:14.446604Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- We can see that low salary hikes of **11 - 14** have been given to  a lot of employees and hence the attrition is high as well.\n- As the **PercentSalaryHike** increases, number of attrited employees decrease!","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize = (15, 5))\nax = sns.countplot('StockOptionLevel', data = df1, hue = 'Attrition', palette = colors,edgecolor = 'black')\nfor rect in ax.patches:\n    ax.text(rect.get_x() + rect.get_width() / 2, rect.get_height()+0.01, rect.get_height(), horizontalalignment='center', fontsize = 11)\nplt.legend(['Retained Employees', 'Attrited Employees'])\nplt.title('StockOptionLevel');","metadata":{"execution":{"iopub.status.busy":"2023-11-23T02:18:14.449704Z","iopub.execute_input":"2023-11-23T02:18:14.450144Z","iopub.status.idle":"2023-11-23T02:18:14.765228Z","shell.execute_reply.started":"2023-11-23T02:18:14.450075Z","shell.execute_reply":"2023-11-23T02:18:14.763799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Same story as the **PercentSalaryHike** can be observed.\n- Number of employees reduces as the **StockOptionLevel** increases.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize = (15, 5))\nax = sns.countplot('BusinessTravel', data = df1, hue = 'Attrition', palette = colors,edgecolor = 'black')\nfor rect in ax.patches:\n    ax.text(rect.get_x() + rect.get_width() / 2, rect.get_height()+0.01, rect.get_height(), horizontalalignment='center', fontsize = 11)\nax.set_xticklabels([tf1['BusinessTravel'][k] for k in sorted(df1['BusinessTravel'].unique())])\nplt.legend(['Retained Employees', 'Attrited Employees'])\nplt.title('BusinessTravel');\n\nfig = plt.subplots(nrows = 1,ncols = 3,figsize = (15,15))\n\nfor i in range(len(df1['BusinessTravel'].unique())):\n    plt.subplot(1,3,i+1)\n    l = list(df1.loc[(df1['BusinessTravel'] == i)]['Attrition'].value_counts())\n    circle = [l[0] / sum(l) * 100,l[1] / sum(l) * 100]\n    plt.pie(circle,labels = list(tf1['Attrition'][k] for k in sorted(df1['Attrition'].unique())),autopct = '%1.1f%%',startangle = 90,explode = (0.1,0),\n            colors = colors, wedgeprops = {'edgecolor' : 'black','linewidth': 1,'antialiased' : True})\n    plt.title('BusinessTravel : ' + tf1['BusinessTravel'][i]);","metadata":{"execution":{"iopub.status.busy":"2023-11-23T02:18:14.767391Z","iopub.execute_input":"2023-11-23T02:18:14.768411Z","iopub.status.idle":"2023-11-23T02:18:15.408785Z","shell.execute_reply.started":"2023-11-23T02:18:14.768359Z","shell.execute_reply":"2023-11-23T02:18:15.407412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- We can see that number of employees that **Travel_Rarely** is huge as compared to **Non-Travel** & **Travel_Frequently**.\n- When it comes to attrition rate, **Travel_Frequently** employees have a **25%** probability of being removed from the company.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize = (15, 5))\nax = sns.countplot('PerformanceRating', data = df1, hue = 'Attrition', palette = colors,edgecolor = 'black')\nfor rect in ax.patches:\n    ax.text(rect.get_x() + rect.get_width() / 2, rect.get_height()+0.01, rect.get_height(), horizontalalignment='center', fontsize = 11)\nax.set_xticklabels(['Excellent', 'Outstanding'])\nplt.legend(['Retained Employees', 'Attrited Employees'])\nplt.title('PerformanceRating');\n\nfig = plt.subplots(nrows = 1,ncols = 2,figsize = (10,10))\n\nfor i in range(len(df1['PerformanceRating'].unique())):\n    plt.subplot(1,2,i+1)\n    l = list(df1.loc[(df1['PerformanceRating'] == (i+3))]['Attrition'].value_counts())\n    circle = [l[0] / sum(l) * 100,l[1] / sum(l) * 100]\n    plt.pie(circle,labels = list(tf1['Attrition'][k] for k in sorted(df1['Attrition'].unique())),autopct = '%1.1f%%',startangle = 90,explode = (0.1,0),\n            colors = colors, wedgeprops = {'edgecolor' : 'black','linewidth': 1,'antialiased' : True})\n    plt.title('PerformanceRating : ' + ['Low', 'Good', 'Excellent', 'Outstanding'][i+2]);","metadata":{"execution":{"iopub.status.busy":"2023-11-23T02:18:15.410528Z","iopub.execute_input":"2023-11-23T02:18:15.411027Z","iopub.status.idle":"2023-11-23T02:18:15.939804Z","shell.execute_reply.started":"2023-11-23T02:18:15.410983Z","shell.execute_reply":"2023-11-23T02:18:15.938104Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- As expected, employees have more **Excellent** rating than **Outstanding**. But when it comes to attrition rate, values of **Excellent** & **Outstanding** are very close with **16.1%** & **16.4%**.\n- No data of **Low** & **Good** **PerformanceRating** are recorded.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize = (15, 5))\nax = sns.countplot('EnvironmentSatisfaction', data = df1, hue = 'Attrition', palette = colors,edgecolor = 'black')\nfor rect in ax.patches:\n    ax.text(rect.get_x() + rect.get_width() / 2, rect.get_height()+0.01, rect.get_height(), horizontalalignment='center', fontsize = 11)\nax.set_xticklabels(['Low', 'Medium', 'High', 'Very High'])\nplt.legend(['Retained Employees', 'Attrited Employees'])\nplt.title('EnvironmentSatisfaction');\n\nfig = plt.subplots(nrows = 1,ncols = 4,figsize = (15,15))\n\nfor i in range(len(df1['EnvironmentSatisfaction'].unique())):\n    plt.subplot(1,4,i+1)\n    l = list(df1.loc[(df1['EnvironmentSatisfaction'] == (i+1))]['Attrition'].value_counts())\n    circle = [l[0] / sum(l) * 100,l[1] / sum(l) * 100]\n    plt.pie(circle,labels = list(tf1['Attrition'][k] for k in sorted(df1['Attrition'].unique())),autopct = '%1.1f%%',startangle = 90,explode = (0.1,0),\n            colors = colors, wedgeprops = {'edgecolor' : 'black','linewidth': 1,'antialiased' : True})\n    plt.title('EnvironmentSatisfaction : ' + ['Low', 'Medium', 'High', 'Very High'][i]);","metadata":{"execution":{"iopub.status.busy":"2023-11-23T02:18:15.943134Z","iopub.execute_input":"2023-11-23T02:18:15.944301Z","iopub.status.idle":"2023-11-23T02:18:16.713863Z","shell.execute_reply.started":"2023-11-23T02:18:15.944228Z","shell.execute_reply":"2023-11-23T02:18:16.712346Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- **High** & **Very High** **EnvironmentSatisfaction** values have been noted the most number of times.\n- As expected, they have a low attrition rate as compared to **Low** & **Medium** **EnvironmentSatisfaction**.\n- The attrition rate improves as the **EnvironmentSatisfaction** improves!","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize = (15, 5))\nax = sns.countplot('RelationshipSatisfaction', data = df1, hue = 'Attrition', palette = colors,edgecolor = 'black')\nfor rect in ax.patches:\n    ax.text(rect.get_x() + rect.get_width() / 2, rect.get_height()+0.01, rect.get_height(), horizontalalignment='center', fontsize = 11)\nax.set_xticklabels(['Low', 'Medium', 'High', 'Very High'])\nplt.legend(['Retained Employees', 'Attrited Employees'])\nplt.title('RelationshipSatisfaction');\n\nfig = plt.subplots(nrows = 1,ncols = 4,figsize = (15,15))\n\nfor i in range(len(df1['RelationshipSatisfaction'].unique())):\n    plt.subplot(1,4,i+1)\n    l = list(df1.loc[(df1['RelationshipSatisfaction'] == (i+1))]['Attrition'].value_counts())\n    circle = [l[0] / sum(l) * 100,l[1] / sum(l) * 100]\n    plt.pie(circle,labels = list(tf1['Attrition'][k] for k in sorted(df1['Attrition'].unique())),autopct = '%1.1f%%',startangle = 90,explode = (0.1,0),\n            colors = colors, wedgeprops = {'edgecolor' : 'black','linewidth': 1,'antialiased' : True})\n    plt.title('RelationshipSatisfaction : ' + ['Low', 'Medium', 'High', 'Very High'][i]);","metadata":{"execution":{"iopub.status.busy":"2023-11-23T02:18:16.716041Z","iopub.execute_input":"2023-11-23T02:18:16.716747Z","iopub.status.idle":"2023-11-23T02:18:17.653138Z","shell.execute_reply.started":"2023-11-23T02:18:16.716710Z","shell.execute_reply":"2023-11-23T02:18:17.651707Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- The above visualizations of **RelationshipSatisfaction** is very similar to **EnvironmentSatisfaction**.\n- As the values of **RelationshipSatisfaction** improves, attrition rate reduces. ","metadata":{}},{"cell_type":"markdown","source":"### Finances :\n\n- It includes features that provide information about employee finances!\n\n- List of Features :\n    - **MonthlyIncome**\n    - **HourlyRate**\n    - **DailyRate**\n    - **MonthlyRate**","metadata":{}},{"cell_type":"code","source":"df2['MonthlyIncome_Group'] = [int(i / 1000) for i in df1['MonthlyIncome']]\nv1 = [df2['MonthlyIncome_Group'].value_counts()[i] for i in sorted(df2['MonthlyIncome_Group'].value_counts().index)]\n\nplt.figure(figsize = (15,5))\nax = sns.lineplot(x = sorted(df2['MonthlyIncome_Group'].value_counts().index), y = v1, lw = 2, color = colors[0], marker = 'o', \n                  markersize = 10, markerfacecolor = colors[1], markeredgewidth = 2, markeredgecolor = colors[0], )\nplt.xlabel('MonthlyIncome : Value*1000'); plt.ylabel('Count')\nplt.title(\"MonthlyIncome\");\n\nplt.figure(figsize = (15, 5))\nax = sns.countplot('MonthlyIncome_Group', data = df2, hue = 'Attrition', palette = colors,edgecolor = 'black')\nfor rect in ax.patches:\n    ax.text(rect.get_x() + rect.get_width() / 2, rect.get_height()+0.01, rect.get_height(), horizontalalignment='center', fontsize = 11)\nplt.legend(['Retained Employees', 'Attrited Employees'])\nplt.title('MonthlyIncome');","metadata":{"execution":{"iopub.status.busy":"2023-11-23T02:18:17.655084Z","iopub.execute_input":"2023-11-23T02:18:17.657188Z","iopub.status.idle":"2023-11-23T02:18:18.541624Z","shell.execute_reply.started":"2023-11-23T02:18:17.657118Z","shell.execute_reply":"2023-11-23T02:18:18.540321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- The graph highlights an overall decline in the count of the values.\n- **MonthlyIncome** values between **1000 - 2000** are present in high numbers. Values between **3000 - 4000** comes second with more than 200 values present in this range.","metadata":{}},{"cell_type":"code","source":"df2['HourlyRate_Group'] = [int(i / 10) for i in df1['HourlyRate']]\nv1 = [df2['HourlyRate_Group'].value_counts()[i] for i in sorted(df2['HourlyRate_Group'].value_counts().index)]\n\nplt.figure(figsize = (15,5))\nax = sns.lineplot(x = sorted(df2['HourlyRate_Group'].value_counts().index), y = v1, lw = 2, color = colors[0], marker = 'o', \n                  markersize = 10, markerfacecolor = colors[1], markeredgewidth = 2, markeredgecolor = colors[0], )\nplt.xlabel('HourlyRate : Value*10'); plt.ylabel('Count')\nplt.title(\"HourlyRate\");\n\nplt.figure(figsize = (15, 5))\nax = sns.countplot('HourlyRate_Group', data = df2, hue = 'Attrition', palette = colors,edgecolor = 'black')\nfor rect in ax.patches:\n    ax.text(rect.get_x() + rect.get_width() / 2, rect.get_height()+0.01, rect.get_height(), horizontalalignment='center', fontsize = 11)\nplt.legend(['Retained Employees', 'Attrited Employees'])\nplt.title('HourlyRate');","metadata":{"execution":{"iopub.status.busy":"2023-11-23T02:18:18.543656Z","iopub.execute_input":"2023-11-23T02:18:18.544209Z","iopub.status.idle":"2023-11-23T02:18:19.262414Z","shell.execute_reply.started":"2023-11-23T02:18:18.544172Z","shell.execute_reply":"2023-11-23T02:18:19.260981Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- For **HourlyRate**, values between **30 - 100** are present with a count of more than **175+** each.\n- Attrition rate of these values is also low and very close to each other. \n- For **HourlyRate** of more than **100**, very few values are present and hence attrition is high as well.","metadata":{}},{"cell_type":"code","source":"df2['DailyRate_Group'] = [int(i / 100) for i in df1['DailyRate']]\nv1 = [df2['DailyRate_Group'].value_counts()[i] for i in sorted(df2['DailyRate_Group'].value_counts().index)]\n\nplt.figure(figsize = (15,5))\nax = sns.lineplot(x = sorted(df2['DailyRate_Group'].value_counts().index), y = v1, lw = 2, color = colors[0], marker = 'o', \n                  markersize = 10, markerfacecolor = colors[1], markeredgewidth = 2, markeredgecolor = colors[0], )\nplt.xlabel('DailyRate : Value*100'); plt.ylabel('Count')\nplt.title(\"DailyRate\");\n\nplt.figure(figsize = (15, 5))\nax = sns.countplot('DailyRate_Group', data = df2, hue = 'Attrition', palette = colors,edgecolor = 'black')\nfor rect in ax.patches:\n    ax.text(rect.get_x() + rect.get_width() / 2, rect.get_height()+0.01, rect.get_height(), horizontalalignment='center', fontsize = 11)\nplt.legend(['Retained Employees', 'Attrited Employees'])\nplt.title('DailyRate');","metadata":{"execution":{"iopub.status.busy":"2023-11-23T02:18:19.264294Z","iopub.execute_input":"2023-11-23T02:18:19.264733Z","iopub.status.idle":"2023-11-23T02:18:20.178892Z","shell.execute_reply.started":"2023-11-23T02:18:19.264694Z","shell.execute_reply":"2023-11-23T02:18:20.177326Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Number of attrited employees is close to each other. Certain drop in count of values are present.\n- Values between **600 - 700** have the lowest count.","metadata":{}},{"cell_type":"code","source":"df2['MonthlyRate_Group'] = [int(i / 1000) for i in df1['MonthlyRate']]\nv1 = [df2['MonthlyRate_Group'].value_counts()[i] for i in sorted(df2['MonthlyRate_Group'].value_counts().index)]\n\nplt.figure(figsize = (15,5))\nax = sns.lineplot(x = sorted(df2['MonthlyRate_Group'].value_counts().index), y = v1, lw = 2, color = colors[0], marker = 'o', \n                  markersize = 10, markerfacecolor = colors[1], markeredgewidth = 2, markeredgecolor = colors[0], )\nplt.xlabel('MonthlyRate : Value*1000'); plt.ylabel('Count')\nplt.title(\"MonthlyRate\");\n\nplt.figure(figsize = (15, 5))\nax = sns.countplot('MonthlyRate_Group', data = df2, hue = 'Attrition', palette = colors,edgecolor = 'black')\nfor rect in ax.patches:\n    ax.text(rect.get_x() + rect.get_width() / 2, rect.get_height()+0.01, rect.get_height(), horizontalalignment='center', fontsize = 11)\nplt.legend(['Retained Employees', 'Attrited Employees'])\nplt.title('MonthlyRate');","metadata":{"execution":{"iopub.status.busy":"2023-11-23T02:18:20.181041Z","iopub.execute_input":"2023-11-23T02:18:20.181476Z","iopub.status.idle":"2023-11-23T02:18:21.304521Z","shell.execute_reply.started":"2023-11-23T02:18:20.181439Z","shell.execute_reply":"2023-11-23T02:18:21.300980Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Values of attrited employees are very close to each other. \n- Values between **21000 - 22000** have the highest count.\n\n- We will check the features of **Finances** with **Department** & **JobLevel** features of **Employee Job Information**.","metadata":{}},{"cell_type":"code","source":"fig = plt.subplots(nrows = 2,ncols = 2,figsize = (15,10))\nfor i in range(len(l5)):\n    plt.subplot(2,2,i+1)\n    ax = sns.boxplot(x = 'Department', y = l5[i], data = df1, hue = 'Attrition',palette = colors);\n    plt.legend(['RE', 'AE'])\n    ax.set_xticklabels([tf1['Department'][k] for k in sorted(df1['Department'].unique())])\n    plt.title('Department vs ' + l5[i]);","metadata":{"execution":{"iopub.status.busy":"2023-11-23T02:18:21.306631Z","iopub.execute_input":"2023-11-23T02:18:21.308283Z","iopub.status.idle":"2023-11-23T02:18:23.313718Z","shell.execute_reply.started":"2023-11-23T02:18:21.308212Z","shell.execute_reply":"2023-11-23T02:18:23.312415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- **MonthlyIncome** has too many outlier values. These outliers are probably from the **JobLevel 5** which is low in numbers & attrition rate.\n- For **HourlyRate**, **Research & Development** & **Sales** department pretty much occupy the same range of values for attrition & non-attrition. Range of attrition values for **Human Resources** is very small.\n- Similar to **HourlyRate**, same pattern can be observed for **Research & Development** & **Sales** department for **DailyRate** & **MonthlyRate**.","metadata":{}},{"cell_type":"code","source":"fig = plt.subplots(nrows = 2,ncols = 2,figsize = (15,10))\nfor i in range(len(l5)):\n    plt.subplot(2,2,i+1)\n    ax = sns.boxplot(x = 'JobLevel', y = l5[i], data = df1, hue = 'Attrition',palette = colors);\n    plt.legend(['RE', 'AE'])\n    ax.set_xticklabels(['JobLevel 1', 'JobLevel 2', 'JobLevel 3', 'JobLevel 4', 'JobLevel 5'])\n    plt.title('JobLevel vs ' + l5[i]);","metadata":{"execution":{"iopub.status.busy":"2023-11-23T02:18:23.315596Z","iopub.execute_input":"2023-11-23T02:18:23.316004Z","iopub.status.idle":"2023-11-23T02:18:25.541207Z","shell.execute_reply.started":"2023-11-23T02:18:23.315968Z","shell.execute_reply":"2023-11-23T02:18:25.539811Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- As expected, as the **JobLevel** increases, **MonthlyIncome** increases! The upper limit value of the previous **JobLevel** value is lower than the lower limit value of the succeding **JobLevel** value. \n- Upper limit values of **JobLevel** of **HourlyRate** are very close to each other. It does not display a significant difference to separate out as **MonthlyIncome**.\n- Pretty much same thing can be observed for **DailyRate** & **MonthlyRate**. **JobLevel 5**'s **DailyRate** & **MonthlyRate** upper limit is clearly differentiable.","metadata":{}},{"cell_type":"markdown","source":"# <center><div style=\"font-family: Times New Roman; border-radius : 10px; background-color: #2BAE66; color: #FCF6F5; padding: 12px; line-height: 1;\">Summary of EDA</div></center>\n\n### Summary of Insights / Order / Values of features w.r.t target variable (Attrition) :\n\n\n- **General Employee Information :**\n    \n    - **Age** : 20 - 44\n    - **Gender**: Male > Female\n    - **MaritalStatus** : Single > Married > Divorced\n    - **Education** : Below College > Bachelor > College > Master > Doctor\n    - **DistanceFromHome** : 20 - 24 > 15 - 19 > 25 - 29 > 10 - 14 > 5 - 9\n    - **TotalWorkingYears** : Very high chances during the 1st 10 working years\n    - **NumCompaniesWorked** : High chances during 1st - 2nd job. Chances increase by a huge margin after working in 4th company.\n    \n    \n- **Employee Job Information :**\n    \n    - **EducationField** : Human Resources > Technical Degree > Marketing > Life Sciences > Medical > Other\n    - **Department** : Sales > Human Resources > Reasearch & Development\n    - **JobLevel** : JobLevel 1 > JobLevel 3 > JobLevel 2 > JobLevel 5 > JobLevel 4. We can see that the **JobRoles** with high attrition rate are present in the **JobLevel** with high attrition rate.\n    - **JobRole** : Sales Representative > Laboratory Technician > Human Resources > Sales Executive > Research Scientist > Healthcare Representative = Manufacturing Director > Manager > Research Director\n    - **JobInvolvement** : Low > Medium > High > Very High\n    - **OverTime** : Yes > No\n    - **JobSatisfaction** :  : Low > Medium > High > Very High\n    \n    \n- **Employee Company Information :**\n    \n    - **YearsAtCompany** : 0 - 4 > 5 - 9 > 10 - 14 > \n    - **YearsInCurrentRole** : Some peaks of high attrition values without any pattern is found.\n    - **YearsWithCurrManager** : Some peaks of high attrition values without any pattern is found.\n    - **YearsSinceLastPromotion** : 0 > 1 > 2. Some other peaks are also found with significant values.\n    - **TrainingTimesLastYear** : 0 > 4 > 2 > 3 > 1 > 5 > 6\n    - **WorkLifeBalance** : Bad > Best > Good > Better\n    \n    \n- **Company Features :**\n    \n    - **PercentSalaryHike** : 11 - 14 has the highest attrition rate. As the value increases, number of attrited employees decrease.\n    - **StockOptionLevel** : Number of employees reduces as the StockOptionLevel increases.\n    - **BusinessTravel** : Travel_Frequently > Travel_Rarely > Non-Travel\n    - **PerformanceRating** : Excellent = Outstanding. No values of Low & Good recorded.\n    - **EnvironmentSatisfaction** : Low > Medium > High > Very High\n    - **RelationshipSatisfaction** : Low > High > Medium > Very High\n    \n    \n- **Finances :**\n    \n    - **MonthlyIncome** : 2000 - 3000\n    - **HourlyRate** : 50 - 60. Values are very close to each other.\n    - **DailyRate** : 300 - 400. Values are very close to each other.\n    - **MonthlyRate** : Very close and small peaks are present. \n\n\n**According to the data, these insights / order / range of values can lead to attrition!**","metadata":{}},{"cell_type":"markdown","source":"# <center><div style=\"font-family: Times New Roman; border-radius : 10px; background-color: #2BAE66; color: #FCF6F5; padding: 12px; line-height: 1;\">Feature Engineering</div></center>","metadata":{}},{"cell_type":"markdown","source":"- The dataset is **Unbalanced** with a bias towards **Retained Employees** in a ratio of **5.2 : 1** for **Retained Employees : Attrited Employees**. We will first balance the dataset using **SMOTE Analysis**!\n\n- In order to cope with unbalanced data, there are 2 options :\n\n    - **Undersampling** : Trim down the majority samples of the target variable.\n    - **Oversampling** : Increase the minority samples of the target variable to the majority samples.\n    \n- For best performances, combination of undersampling and oversampling is recommended.\n- First, we will undersample the majority samples and it is followed by oversampling minority samples.\n- For data balancing, we will use **imblearn**.\n- **PIP statement** : pip install imbalanced-learn","metadata":{}},{"cell_type":"markdown","source":"### Data Balancing using SMOTE :","metadata":{}},{"cell_type":"code","source":"import imblearn\nfrom collections import Counter\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.pipeline import Pipeline","metadata":{"execution":{"iopub.status.busy":"2023-11-23T02:18:25.543080Z","iopub.execute_input":"2023-11-23T02:18:25.543588Z","iopub.status.idle":"2023-11-23T02:18:25.959231Z","shell.execute_reply.started":"2023-11-23T02:18:25.543541Z","shell.execute_reply":"2023-11-23T02:18:25.957990Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cols = list(df1.columns)\ncols.remove('Attrition')\n\nover = SMOTE(sampling_strategy = 0.85)\nunder = RandomUnderSampler(sampling_strategy = 0.1)\nf1 = df1.loc[:,cols]\nt1 = df1.loc[:,'Attrition']\n\nsteps = [('over', over)]\npipeline = Pipeline(steps=steps)\nf1, t1 = pipeline.fit_resample(f1, t1)\nCounter(t1)","metadata":{"execution":{"iopub.status.busy":"2023-11-23T02:18:25.960791Z","iopub.execute_input":"2023-11-23T02:18:25.961206Z","iopub.status.idle":"2023-11-23T02:18:26.010052Z","shell.execute_reply.started":"2023-11-23T02:18:25.961170Z","shell.execute_reply":"2023-11-23T02:18:26.008312Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Calculation for Data Balancing :\n\n- **Sampling Strategy** : It is a ratio which is the common paramter for oversampling and undersampling.\n- **Sampling Strategy** : **( Samples of Minority Class ) / ( Samples of Majority Class )**\n\n\n- In this case,\n\n    - **Majority Class : Retained Employees** : 1233 samples\n    - **Minority Class : Attrited Employees** : 237 samples\n\n\n### Oversampling : Increase the minority class samples\n\n- Sampling_Strategy = 0.85\n- 0.85 = ( Minority Class Samples ) / 1233\n- After oversampling, \n\n    - **Majority Class : Retained Employees** : 1233 samples\n    - **Minority Class : Attrited Employees** : 1048 samples\n    \n\n- Final Class Samples :\n\n    - **Majority Class : Retained Employees** : 1233 samples\n    - **Minority Class : Attrited Employees** : 1048 samples\n\n\n- Here, we balance the data by increasing the minority group to majority group. In this case we only increase the minority data points as the data is very less.\n- For imbalanced datasets, we **duplicate the data** to deal with the potential bias in the predictions. \n- Due to this duplication process, we are using **synthetic data** for modeling purposes to ensure that the predictions are not skewed towards the majority target class value.\n- Thus, evaluating models using **accuracy** will be misleading. Instead, we will go for **confusion matrix, ROC-AUC graph and ROC-AUC score** for model evaluation.","metadata":{}},{"cell_type":"markdown","source":"### Data Leakage : \n\n- **Data Leakage** is the problem when the information outside the training data is used for model creation. It is one of the most ignored problem.\n- In order to create robust models, solving data leakage is a must! Creation of overly optimistic models which are practically useless & cannot be used in production have become common.\n- Model performance degrades when **Data Leakage** is not dealt with & the model is sent online. It is a difficult concept to understand because it seems quite trivial.\n- Typical approach used is transforming / modifying the entire dataset by filling NAN values with mean, median & mode, standardisation, normalization, etc.\n- When we execute the above process in order to make the dataset ready for modeling, we use the values from the entire dataset & thus indirectly provide information from the to-be test data i.e outside of the training data.\n- Thus, in order to avoid **Data Leakage**, it is advised to use train-test-split before any transformations. Execute the transformations according to the training data for the training as well as test data. Use of k-fold cross validation is also suggested!","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(f1, t1, test_size = 0.15, random_state = 2)","metadata":{"execution":{"iopub.status.busy":"2023-11-23T02:18:26.011798Z","iopub.execute_input":"2023-11-23T02:18:26.012720Z","iopub.status.idle":"2023-11-23T02:18:26.028178Z","shell.execute_reply.started":"2023-11-23T02:18:26.012668Z","shell.execute_reply":"2023-11-23T02:18:26.025979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Correlation Matrix :","metadata":{}},{"cell_type":"code","source":"x_train_test = x_train.copy(deep = True)\nx_train_test['Attrition'] = y_train","metadata":{"execution":{"iopub.status.busy":"2023-11-23T02:18:26.032405Z","iopub.execute_input":"2023-11-23T02:18:26.034471Z","iopub.status.idle":"2023-11-23T02:18:26.047174Z","shell.execute_reply.started":"2023-11-23T02:18:26.034380Z","shell.execute_reply":"2023-11-23T02:18:26.044917Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- In order to visualize the correlation matrix, we create a new dataframe that contains values from **x_train** & **y_train**.\n- Thus, we reject anything outside the training data to avoid data leakage.","metadata":{}},{"cell_type":"code","source":"corr = x_train_test.corrwith(x_train_test['Attrition']).sort_values(ascending = False).to_frame()\ncorr.columns = ['Attrition']\nplt.subplots(figsize = (7,10))\nsns.heatmap(corr,annot = True,cmap = colors,linewidths = 0.4,linecolor = 'black');\nplt.title('Correlation w.r.t Attrition');","metadata":{"execution":{"iopub.status.busy":"2023-11-23T02:18:26.051020Z","iopub.execute_input":"2023-11-23T02:18:26.052571Z","iopub.status.idle":"2023-11-23T02:18:26.819617Z","shell.execute_reply.started":"2023-11-23T02:18:26.052495Z","shell.execute_reply":"2023-11-23T02:18:26.818168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- None of the features display a strong positive or negative correlation with **Attrition**.\n- Most of the features have values between [-0.3 - 0.14].  ","metadata":{}},{"cell_type":"markdown","source":"### Feature Selection for Categorical Features :","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import mutual_info_classif,chi2","metadata":{"execution":{"iopub.status.busy":"2023-11-23T02:18:26.821911Z","iopub.execute_input":"2023-11-23T02:18:26.822376Z","iopub.status.idle":"2023-11-23T02:18:26.839513Z","shell.execute_reply.started":"2023-11-23T02:18:26.822338Z","shell.execute_reply":"2023-11-23T02:18:26.838275Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Mutual Information Test :","metadata":{}},{"cell_type":"code","source":"features = x_train.loc[:,categorical_features]\ntarget = pd.DataFrame(y_train)\n\nbest_features = SelectKBest(score_func = mutual_info_classif,k = 'all')\nfit = best_features.fit(features,target)\n\nfeatureScores = pd.DataFrame(data = fit.scores_,index = list(features.columns),columns = ['Mutual Information Score']) \n\nplt.subplots(figsize = (5,5))\nsns.heatmap(featureScores.sort_values(ascending = False,by = 'Mutual Information Score'),annot = True,cmap = colors,linewidths = 0.4,linecolor = 'black',fmt = '.2f');\nplt.title('Selection of Categorical Features');","metadata":{"execution":{"iopub.status.busy":"2023-11-23T02:18:26.841188Z","iopub.execute_input":"2023-11-23T02:18:26.841573Z","iopub.status.idle":"2023-11-23T02:18:27.422580Z","shell.execute_reply.started":"2023-11-23T02:18:26.841536Z","shell.execute_reply":"2023-11-23T02:18:27.420963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Mutual Information Score of **Attrition** with categorical features display very low scores.\n- According to the above scores, none of the features should be selected for modeling.","metadata":{}},{"cell_type":"markdown","source":"#### Chi Squared Test :","metadata":{}},{"cell_type":"code","source":"features = x_train.loc[:,categorical_features]\ntarget = pd.DataFrame(y_train)\n\nbest_features = SelectKBest(score_func = chi2,k = 'all')\nfit = best_features.fit(features,target)\n\nfeatureScores = pd.DataFrame(data = fit.scores_,index = list(features.columns),columns = ['Chi Squared Score']) \n\nplt.subplots(figsize = (5,5))\nsns.heatmap(featureScores.sort_values(ascending = False,by = 'Chi Squared Score'),annot = True,cmap = colors,linewidths = 0.4,linecolor = 'black',fmt = '.2f');\nplt.title('Selection of Categorical Features');","metadata":{"execution":{"iopub.status.busy":"2023-11-23T02:18:27.424764Z","iopub.execute_input":"2023-11-23T02:18:27.425407Z","iopub.status.idle":"2023-11-23T02:18:27.843346Z","shell.execute_reply.started":"2023-11-23T02:18:27.425349Z","shell.execute_reply":"2023-11-23T02:18:27.841681Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- From the above **Chi Squared Score Test**, we will drop the following features : **PerformanceRating**, **Department**, **JobRole**, **EducationField**, **BusinessTravel**, **MaritalStatus** & **Gender**.","metadata":{}},{"cell_type":"markdown","source":"### Feature Selection for Numerical Features :","metadata":{}},{"cell_type":"markdown","source":"#### ANOVA Test :","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_selection import f_classif\n\nfeatures = x_train.loc[:,discrete_features]\ntarget = pd.DataFrame(y_train)\n\nbest_features = SelectKBest(score_func = f_classif,k = 'all')\nfit = best_features.fit(features,target)\n\nfeatureScores = pd.DataFrame(data = fit.scores_,index = list(features.columns),columns = ['ANOVA Score']) \n\nplt.subplots(figsize = (5,5))\nsns.heatmap(featureScores.sort_values(ascending = False,by = 'ANOVA Score'),annot = True,cmap = colors,linewidths = 0.4,linecolor = 'black',fmt = '.2f');\nplt.title('Selection of Numerical Features');","metadata":{"execution":{"iopub.status.busy":"2023-11-23T02:18:27.845121Z","iopub.execute_input":"2023-11-23T02:18:27.845480Z","iopub.status.idle":"2023-11-23T02:18:28.253605Z","shell.execute_reply.started":"2023-11-23T02:18:27.845447Z","shell.execute_reply":"2023-11-23T02:18:28.252120Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- From the above **ANOVA Score Test**, we will drop the following features : **MonthlyRate**, **HourlyRate**, **NumCompaniesWorked**, **PercentSalaryHike**, **YearsSinceLastPromotion**, **DistanceFromHome** & **DailyRate**.\n- We ready the datasets for data scaling by dropping the features based on the above statistical tests.","metadata":{}},{"cell_type":"code","source":"x_train = x_train.drop(columns = ['MonthlyRate', 'HourlyRate', 'NumCompaniesWorked', 'PercentSalaryHike', \n                                  'YearsSinceLastPromotion', 'DistanceFromHome','DailyRate',\n                                  'PerformanceRating', 'Department', 'JobRole', 'EducationField', \n                                  'BusinessTravel', 'MaritalStatus' ,'Gender'])\n\nx_test = x_test.drop(columns = ['MonthlyRate', 'HourlyRate', 'NumCompaniesWorked', 'PercentSalaryHike', \n                                  'YearsSinceLastPromotion', 'DistanceFromHome','DailyRate',\n                                  'PerformanceRating', 'Department', 'JobRole', 'EducationField', \n                                  'BusinessTravel', 'MaritalStatus' ,'Gender'])","metadata":{"execution":{"iopub.status.busy":"2023-11-23T02:18:28.255830Z","iopub.execute_input":"2023-11-23T02:18:28.256318Z","iopub.status.idle":"2023-11-23T02:18:28.267024Z","shell.execute_reply.started":"2023-11-23T02:18:28.256272Z","shell.execute_reply":"2023-11-23T02:18:28.265898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data Scaling :","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler,StandardScaler\nmms = MinMaxScaler() # Normalization\nss = StandardScaler() # Standardization\n\n# Normalization\nx_train['MonthlyIncome'] = mms.fit_transform(x_train[['MonthlyIncome']]); x_test['MonthlyIncome'] = mms.transform(x_test[['MonthlyIncome']])\nx_train['TotalWorkingYears'] = mms.fit_transform(x_train[['TotalWorkingYears']]); x_test['TotalWorkingYears'] = mms.transform(x_test[['TotalWorkingYears']])\nx_train['YearsAtCompany'] = mms.fit_transform(x_train[['YearsAtCompany']]); x_test['YearsAtCompany'] = mms.transform(x_test[['YearsAtCompany']])\nx_train['YearsInCurrentRole'] = mms.fit_transform(x_train[['YearsInCurrentRole']]); x_test['YearsInCurrentRole'] = mms.transform(x_test[['YearsInCurrentRole']])\nx_train['YearsWithCurrManager'] = mms.fit_transform(x_train[['YearsWithCurrManager']]); x_test['YearsWithCurrManager'] = mms.transform(x_test[['YearsWithCurrManager']])\n\n# Standardization\nx_train['Age'] = ss.fit_transform(x_train[['Age']]); x_test['Age'] = ss.transform(x_test[['Age']])\nx_train['Education'] = ss.fit_transform(x_train[['Education']]); x_test['Education'] = ss.transform(x_test[['Education']])\nx_train['EnvironmentSatisfaction'] = ss.fit_transform(x_train[['EnvironmentSatisfaction']]); x_test['EnvironmentSatisfaction'] = ss.transform(x_test[['EnvironmentSatisfaction']])\nx_train['JobInvolvement'] = ss.fit_transform(x_train[['JobInvolvement']]); x_test['JobInvolvement'] = ss.transform(x_test[['JobInvolvement']])\nx_train['JobLevel'] = ss.fit_transform(x_train[['JobLevel']]); x_test['JobLevel'] = ss.transform(x_test[['JobLevel']])\nx_train['JobSatisfaction'] = ss.fit_transform(x_train[['JobSatisfaction']]); x_test['JobSatisfaction'] = ss.transform(x_test[['JobSatisfaction']])\nx_train['OverTime'] = ss.fit_transform(x_train[['OverTime']]); x_test['OverTime'] = ss.transform(x_test[['OverTime']])\nx_train['RelationshipSatisfaction'] = ss.fit_transform(x_train[['RelationshipSatisfaction']]); x_test['RelationshipSatisfaction'] = ss.transform(x_test[['RelationshipSatisfaction']])\nx_train['StockOptionLevel'] = ss.fit_transform(x_train[['StockOptionLevel']]); x_test['StockOptionLevel'] = ss.transform(x_test[['StockOptionLevel']])\nx_train['TrainingTimesLastYear'] = ss.fit_transform(x_train[['TrainingTimesLastYear']]); x_test['TrainingTimesLastYear'] = ss.transform(x_test[['TrainingTimesLastYear']])\nx_train['WorkLifeBalance'] = ss.fit_transform(x_train[['WorkLifeBalance']]); x_test['WorkLifeBalance'] = ss.transform(x_test[['WorkLifeBalance']])","metadata":{"execution":{"iopub.status.busy":"2023-11-23T02:18:28.269466Z","iopub.execute_input":"2023-11-23T02:18:28.270367Z","iopub.status.idle":"2023-11-23T02:18:28.398636Z","shell.execute_reply.started":"2023-11-23T02:18:28.270306Z","shell.execute_reply":"2023-11-23T02:18:28.396965Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Machine learning model does not understand the units of the values of the features. It treats the input just as a simple number but does not understand the true meaning of that value. Thus, it becomes necessary to scale the data.\n\n- We have 2 options for data scaling : \n    \n    1) **Normalization** \n    \n    2) **Standardization**. \n\n\n- As most of the algorithms assume the data to be normally (Gaussian) distributed, **Normalization** is done for features whose data does not display normal distribution and **standardization** is carried out for features that are normally distributed but the range of values is huge or small as compared to other features.\n\n- From the above transformation, we fit the data on the training data and transform the test data from information based on the training data. If we check the formulas of the **Normalization** & **Standardization**, we use **mean**, **standard deviation**, **min & max** values.\n\n- Thus if these above statistical parameters are calculated using the complete dataset, then we are sharing the values from the **to-be test data** and thus sharing this **to-be test data** with the training data and cause **Data Leakage**.","metadata":{}},{"cell_type":"markdown","source":"# <center><div style=\"font-family: Times New Roman; border-radius : 10px; background-color: #2BAE66; color: #FCF6F5; padding: 12px; line-height: 1;\">Modeling</div></center>","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import plot_roc_curve\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.metrics import precision_recall_curve","metadata":{"execution":{"iopub.status.busy":"2023-11-23T02:18:28.400533Z","iopub.execute_input":"2023-11-23T02:18:28.400898Z","iopub.status.idle":"2023-11-23T02:18:28.408923Z","shell.execute_reply.started":"2023-11-23T02:18:28.400865Z","shell.execute_reply":"2023-11-23T02:18:28.407500Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Selecting the features from the above conducted tests and splitting the data into **85 - 15 train - test** groups.","metadata":{}},{"cell_type":"code","source":"def model(classifier,x_train,y_train,x_test,y_test):\n    \n    classifier.fit(x_train,y_train)\n    prediction = classifier.predict(x_test)\n    cv = RepeatedStratifiedKFold(n_splits = 10,n_repeats = 3,random_state = 1)\n    print(\"Cross Validation Score : \",'{0:.2%}'.format(cross_val_score(classifier,x_train,y_train,cv = cv,scoring = 'roc_auc').mean()))\n    print(\"ROC_AUC Score : \",'{0:.2%}'.format(roc_auc_score(y_test,prediction)))\n    plot_roc_curve(classifier, x_test,y_test)\n    plt.title('ROC_AUC_Plot')\n    plt.show()\n\ndef model_evaluation(classifier,x_test,y_test):\n    \n    # Confusion Matrix\n    cm = confusion_matrix(y_test,classifier.predict(x_test))\n    names = ['True Neg','False Pos','False Neg','True Pos']\n    counts = [value for value in cm.flatten()]\n    percentages = ['{0:.2%}'.format(value) for value in cm.flatten()/np.sum(cm)]\n    labels = [f'{v1}\\n{v2}\\n{v3}' for v1, v2, v3 in zip(names,counts,percentages)]\n    labels = np.asarray(labels).reshape(2,2)\n    sns.heatmap(cm,annot = labels,cmap = colors,fmt ='')\n    \n    # Classification Report\n    print(classification_report(y_test,classifier.predict(x_test)))","metadata":{"execution":{"iopub.status.busy":"2023-11-23T02:18:28.410888Z","iopub.execute_input":"2023-11-23T02:18:28.411422Z","iopub.status.idle":"2023-11-23T02:18:28.427724Z","shell.execute_reply.started":"2023-11-23T02:18:28.411359Z","shell.execute_reply":"2023-11-23T02:18:28.426104Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1] XGBoostClassifier :","metadata":{}},{"cell_type":"code","source":"from xgboost import XGBClassifier","metadata":{"execution":{"iopub.status.busy":"2023-11-23T02:18:28.429624Z","iopub.execute_input":"2023-11-23T02:18:28.430167Z","iopub.status.idle":"2023-11-23T02:18:28.549558Z","shell.execute_reply.started":"2023-11-23T02:18:28.430113Z","shell.execute_reply":"2023-11-23T02:18:28.548108Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"classifier_xgb = XGBClassifier(learning_rate= 0.01,max_depth = 3,n_estimators = 1000)","metadata":{"execution":{"iopub.status.busy":"2023-11-23T02:18:28.551299Z","iopub.execute_input":"2023-11-23T02:18:28.551694Z","iopub.status.idle":"2023-11-23T02:18:28.558937Z","shell.execute_reply.started":"2023-11-23T02:18:28.551658Z","shell.execute_reply":"2023-11-23T02:18:28.557588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model(classifier_xgb,x_train.values,y_train.values,x_test.values,y_test.values)\nmodel_evaluation(classifier_xgb,x_test.values,y_test.values)","metadata":{"execution":{"iopub.status.busy":"2023-11-23T02:18:28.561031Z","iopub.execute_input":"2023-11-23T02:18:28.561533Z","iopub.status.idle":"2023-11-23T02:21:11.991220Z","shell.execute_reply.started":"2023-11-23T02:18:28.561496Z","shell.execute_reply":"2023-11-23T02:21:11.989637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2] LGBMClassifier :","metadata":{}},{"cell_type":"code","source":"from lightgbm import LGBMClassifier","metadata":{"execution":{"iopub.status.busy":"2023-11-23T02:21:11.993424Z","iopub.execute_input":"2023-11-23T02:21:11.994022Z","iopub.status.idle":"2023-11-23T02:21:13.045943Z","shell.execute_reply.started":"2023-11-23T02:21:11.993967Z","shell.execute_reply":"2023-11-23T02:21:13.044494Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"classifier_lgbm = LGBMClassifier(learning_rate= 0.01,max_depth = 3,n_estimators = 1000)","metadata":{"execution":{"iopub.status.busy":"2023-11-23T02:21:13.048034Z","iopub.execute_input":"2023-11-23T02:21:13.048640Z","iopub.status.idle":"2023-11-23T02:21:13.055811Z","shell.execute_reply.started":"2023-11-23T02:21:13.048597Z","shell.execute_reply":"2023-11-23T02:21:13.054215Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model(classifier_lgbm,x_train.values,y_train.values,x_test.values,y_test.values)\nmodel_evaluation(classifier_lgbm,x_test.values,y_test.values)","metadata":{"execution":{"iopub.status.busy":"2023-11-23T02:21:13.057948Z","iopub.execute_input":"2023-11-23T02:21:13.058485Z","iopub.status.idle":"2023-11-23T02:21:26.997209Z","shell.execute_reply.started":"2023-11-23T02:21:13.058436Z","shell.execute_reply":"2023-11-23T02:21:26.995602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3] Decision Tree Classifier :","metadata":{}},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier","metadata":{"execution":{"iopub.status.busy":"2023-11-23T02:21:26.998975Z","iopub.execute_input":"2023-11-23T02:21:26.999482Z","iopub.status.idle":"2023-11-23T02:21:27.007264Z","shell.execute_reply.started":"2023-11-23T02:21:26.999447Z","shell.execute_reply":"2023-11-23T02:21:27.005141Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"classifier_dt = DecisionTreeClassifier(random_state = 1000,max_depth = 4,min_samples_leaf = 1)","metadata":{"execution":{"iopub.status.busy":"2023-11-23T02:21:27.009961Z","iopub.execute_input":"2023-11-23T02:21:27.010575Z","iopub.status.idle":"2023-11-23T02:21:27.023490Z","shell.execute_reply.started":"2023-11-23T02:21:27.010521Z","shell.execute_reply":"2023-11-23T02:21:27.022060Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model(classifier_dt,x_train.values,y_train.values,x_test.values,y_test.values)\nmodel_evaluation(classifier_dt,x_test.values,y_test.values)","metadata":{"execution":{"iopub.status.busy":"2023-11-23T02:21:27.025122Z","iopub.execute_input":"2023-11-23T02:21:27.025636Z","iopub.status.idle":"2023-11-23T02:21:27.772426Z","shell.execute_reply.started":"2023-11-23T02:21:27.025599Z","shell.execute_reply":"2023-11-23T02:21:27.770996Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4] RandomForest Classifier :","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier","metadata":{"execution":{"iopub.status.busy":"2023-11-23T02:21:27.773841Z","iopub.execute_input":"2023-11-23T02:21:27.774190Z","iopub.status.idle":"2023-11-23T02:21:27.782009Z","shell.execute_reply.started":"2023-11-23T02:21:27.774159Z","shell.execute_reply":"2023-11-23T02:21:27.780463Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"classifier_rf = RandomForestClassifier(max_depth = 4,random_state = 0)","metadata":{"execution":{"iopub.status.busy":"2023-11-23T02:21:27.783819Z","iopub.execute_input":"2023-11-23T02:21:27.784206Z","iopub.status.idle":"2023-11-23T02:21:27.799489Z","shell.execute_reply.started":"2023-11-23T02:21:27.784172Z","shell.execute_reply":"2023-11-23T02:21:27.797728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model(classifier_rf,x_train.values,y_train.values,x_test.values,y_test.values)\nmodel_evaluation(classifier_rf,x_test.values,y_test.values)","metadata":{"execution":{"iopub.status.busy":"2023-11-23T02:21:27.801752Z","iopub.execute_input":"2023-11-23T02:21:27.802205Z","iopub.status.idle":"2023-11-23T02:21:37.870905Z","shell.execute_reply.started":"2023-11-23T02:21:27.802168Z","shell.execute_reply":"2023-11-23T02:21:37.869459Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### ML Alogrithm Results Table :\n\n#### Results Table for models based on Statistical Test : \n\n|Sr. No.|ML Algorithm|Cross Validation Score|ROC AUC Score|F1 Score (Attrition)| F1 Score (No Attrition)|\n|-|-|-|-|-|-|\n|1|XGB Classifier|91.92%|88.25%|87%|89%|\n|2|LGBM Regression|92.14%|88.44%|88%|90%|\n|3|Decision Tree Classifier|80.22%|78.68%|77%|81%|\n|4|RandomForest Classifier|87.62%|81.72%|80%|84%|","metadata":{}},{"cell_type":"markdown","source":"# <center><div style=\"font-family: Times New Roman; border-radius : 10px; background-color: #2BAE66; color: #FCF6F5; padding: 12px; line-height: 1;\">Conclusion</div></center>\n\n- This is an extensive & huge dataset that poses the problem of binary classification with multiple text and numerical features that are categorical & discrete in nature.\n\n\n- This is another imbalanced dataset that needs to be dealt using **SMOTE analysis**. It provides us an with a plethora of opportunities to work on EDA using visualizations to gain insights. Grouping the features together is key!\n\n\n- We also aim to make the models robust by solving the **Data Leakage** problem.  Model performances are good as well. It also gives us to chances to learn about varied code optimization techniques as well.","metadata":{}},{"cell_type":"markdown","source":"### References :\n- [Image Source](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQoKWCb0545g__QBdCLP8_7IUmIjC2GFZtzBQ&usqp=CAU)","metadata":{}},{"cell_type":"markdown","source":"# <center><div style=\"font-family: Times New Roman; background-color: #2BAE66; color: #FCF6F5; padding: 12px; line-height: 1;\">Please Upvote if you like the work!</div><div style=\"font-family: Times New Roman; background-color: #2BAE66; color: #FCF6F5; padding: 12px; line-height: 1;\">Any Sort of Feedback is Appreciated!</div><div style=\"font-family: Times New Roman; background-color: #2BAE66; color: #FCF6F5; padding: 12px; line-height: 1;\">Thank You!</div></center>","metadata":{}}]}